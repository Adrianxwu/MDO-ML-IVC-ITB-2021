{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 7 Surrogate Modelling for Students.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zhpiYf49260g"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adrianxwu/MDO-ML-IVC-ITB-2021/blob/main/Tutorial_7_Surrogate_Modelling_for_Students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhpiYf49260g"
      },
      "source": [
        "# **TUTORIAL 7: SURROGATE MODELLING AND MACHINE LEARNING: MODERN APPROXIMATION TOOLS**\n",
        "\n",
        "**3 August 2021**\n",
        "\n",
        "<img src=\"https://mdoml2021.ftmd.itb.ac.id/wp-content/uploads/2021/06/image.png\" width=\"15%\"><br>\n",
        "<small>2021 Â© MDOML IVC ITB</small>\n",
        "\n",
        "\n",
        "Based on the lecture material by: Pramudita Satria Palar, Ph.D. (Institut Teknologi Bandung, Indonesia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5UYEJzOWuJQ"
      },
      "source": [
        "Please run the following code first to import all required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CybPg5dyCxZ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.optimize as optimize\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVHM6B6DEFQ6"
      },
      "source": [
        "# Introduction\n",
        "The task of any surrogate model is to approximate an unknown black box function $f(\\mathbf{x})$, where $\\mathbf{x}=\\{x_{1},x_{2},\\ldots,x_{m}\\}$ and $m$ is the dimensionality of the input variable, with an approximation function $\\hat{f}(\\mathbf{x})$. The approximation function can take any form, e.g. polynomial or radial basis function. \n",
        "\n",
        "First, we need to prepare the design of experiment (DoE) consisting of $n$ sampling points, i.e., $\\mathcal{X}=\\{\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\ldots,\\mathbf{x}^{(n)}\\}^{T}$. The responses of $\\mathcal{X}$ can be calculated by physical or computer experiments to yield $\\mathbf{y}=\\{y^{(1)},y^{(2)},\\ldots,y^{(n)}\\}^{T} = \\{f(\\mathbf{x}^{(1)}),f(\\mathbf{x}^{(2)}),\\ldots,f(\\mathbf{x}^{(n)})\\}^{T}$. There are several DoE technique that we can use to generate $\\mathcal{X}$ including latin hypercube sampling, Sobol sequence, and Halton sequence.\n",
        "\n",
        "A good surrogate model should approximate the true function well, i.e., $f(\\mathbf{x})\\approx \\hat{f}(\\mathbf{x})$. However, depending on the application, sometimes the surrogate is only required to be accurate on interesting regions (e.g. accurate in the basin of global optimum). \n",
        "\n",
        "In this tutorial, we will learn the basic concepts of two surrogate models, namely, **polynomial regression (PR)** and **radial basis function (RBF)**. We think that the aforementioned methods are suitable for educational purpose for the students to study other, possibly more advanced, surrogate models. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfuVxEVuELNZ"
      },
      "source": [
        "# Generalized Linear Model\n",
        "Both PR and RBF are **generalized linear model**, that is\n",
        "\n",
        "> $\\hat{f}(\\mathbf{x})=\\boldsymbol{\\alpha}^{T}\\boldsymbol{\\varphi}=\\sum_{i=1}^{p}\\alpha_{i} \\phi_{i}(\\mathbf{x})$\n",
        "\n",
        "where $\\boldsymbol{\\alpha}=\\{\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{p}\\}^{T}$ is the vector of coefficients, $p$ is the size of basis functions set, and $\\boldsymbol{\\varphi}$ is the vector of basis functions. The basis functions $\\boldsymbol{\\varphi}$ differ for PR and RBF, as will be discussed next. Estimation of $\\boldsymbol{\\alpha}$ can be done by various means such as by using least-squares or maximizing the model likelihood. In both RBF and PR, $\\boldsymbol{\\alpha}$  is obtained by solving a system of linear equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OEdvyqAFq7g"
      },
      "source": [
        "# Latin hypercube sampling\n",
        "Through this tutorial, we will use the latin hypercube sampling (LHS) method to generate the sampling points. The function is ```sampling_rlh(Nsamp=, dimen=, edges=)```, where ```Nsamp``` is the number of samples, ```dimen``` is the input dimensionality, and ```edges``` determines whether we want the outermost samples to be located on the bounds or not (i.e, set ```edges=1``` if you want to do so and ```edges=0``` otherwise) \n",
        "\n",
        " as defined in the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX5luNeKSUK7"
      },
      "source": [
        "def sampling_rlh(\n",
        "    Nsamp, dimen, edges = 0\n",
        "):\n",
        "\n",
        "  X = np.zeros(shape=[Nsamp, dimen])\n",
        "\n",
        "  for i in range(dimen):\n",
        "     X[:, i] = np.random.permutation(Nsamp) + 1\n",
        "\n",
        "  if edges == 1:\n",
        "      X = (X - 1)/(Nsamp -1)\n",
        "  else:\n",
        "      X = (X - 0.5)/Nsamp\n",
        "\n",
        "  return X\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmD9WZPA59E_"
      },
      "source": [
        "Let's try to generate an example of sampling points based on LHS:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS71ftFe6DqE"
      },
      "source": [
        "Xex = sampling_rlh(Nsamp=20,dimen=2,edges=0) # An example of LHS-generated sampling points\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(Xex[:,0],Xex[:,1],s=20,c='b')\n",
        "plt.xlabel('$x_{1}$')\n",
        "plt.ylabel('$x_{2}$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voaoJWSnZ1iH"
      },
      "source": [
        "# Polynomial Regression\n",
        "\n",
        "The standard PR uses monomial basis function in the formulation (i.e., $x,x^{2},x^{3},\\ldots$), which is something that you are already familiar with. In this tutorial, we will extend the one-dimensional PR into multidimensional approximation with $m$ variables. \n",
        "\n",
        "In this paper, we will study polynomial regression through the polynomial chaos expansion (PCE) method. Notice that there exists an intrusive version of PCE (hence, it is not a surrogate model). \n",
        "\n",
        "> $\\hat{f}(\\mathbf{x})=\\sum_{j=0}^{p} \\alpha_{j} \\Psi_{j}(\\mathbf{x})$\n",
        "\n",
        "where $\\boldsymbol{\\alpha}$ is now the vector of PCE coefficients and $\\Psi$ is a multidimensional orthogonal polynomial basis. Notice that the index starts from zero for convenience since $\\Psi_{0}(\\mathbf{x})$. The nice thing about PCe is that several quantities of interests such as Sobol indices, mean, and variance of the output can be directly calculated from the coefficients. Orthogonal polynomials are polynomials that satisfy the following relation:\n",
        "> $\\int_{\\Omega}\\Psi_{i}(\\mathbf{x})\\Psi_{j}(\\mathbf{x}) d \\mathbf{x} = \\delta_{ij}$\n",
        " \n",
        "where $\\delta_{ij}$ is the kronecker delta (i.e., $\\delta_{ij}=1$ if $i=j$ and $\\delta_{ij}=0$ if $i \\neq j$).\n",
        "\n",
        "The coefficients $\\boldsymbol{\\alpha}$ can be obtained by solving the following system of linear equation:\n",
        "> $\\boldsymbol{F} \\boldsymbol{\\alpha} = \\mathbf{y}$\n",
        "\n",
        "where $F$ is an $n \\times p $ regression matrix with its $(i,j)$-th component is $F_{i,j} = \\Psi_{j}(\\mathbf{x}^{(i)})$. Thus, $\\boldsymbol{\\alpha}=\\boldsymbol{F}^{-1} \\mathbf{y}$. Notice that the regression matrix is constructed by using the experimental design $\\mathcal{X}$ and the defined polynomial basis set.\n",
        "\n",
        "In this tutorial, we will use Legendre polynomials in our PCE. The reason why Legendre polynomials are used in this paper is due to the bounded input space. \n",
        "\n",
        "In practice, it is more often that we have $\\mathcal{X}$ first and we need to define the most suitable polynomial basis set. We will use LOOCV to find the polynomial basis set that yields the lowest CV error. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0geIf7-MH9b"
      },
      "source": [
        "---\n",
        "## Polynomial Indexing\n",
        "\n",
        "While it is not obvious from the theoritical explanation, the code implementation for each polynomial terms is a bit complex.\n",
        "\n",
        "The output of this polynomial indexing problem is to create the following matrix:\n",
        "\n",
        "$ \\begin{pmatrix} q^{1}_{0} & ... & q^{m}_{0} \\\\\\ \\vdots &  & \\vdots \\\\\\ q^{1}_{p} & ... & q^{m}_{p} \\end{pmatrix}$\n",
        "\n",
        "where $q^{i}_{j}$ is the power of the polynomial term in the $i$-th dimension and $j$-th polynomial basis.\n",
        "\n",
        "The cell below is the implementation on how to index all the polynomial terms. The main subroutine that we will use is ```total_trunc(pmax, dimen)```, which generates polynomial indices based on **total order truncation**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrrmj5dOfwvR",
        "cellView": "form"
      },
      "source": [
        "#@title Generating polynomial indices\n",
        "def nchoosek(n, k):\n",
        "    C = itertools.combinations(n, k)\n",
        "    C = np.array(list(C))\n",
        "      \n",
        "    return C\n",
        "\n",
        "def repmat(A, rowx, colx):\n",
        "    row = A.shape[0]\n",
        "    col = A.shape[1]\n",
        "    X = np.zeros(shape=[row*rowx, col*colx])\n",
        "    Xcol = np.zeros(shape=[row, col*colx])\n",
        "\n",
        "    for i in range(colx):\n",
        "      Xcol[:, (col*i):(col*(i+1))] = A\n",
        "\n",
        "    for i in range(rowx):\n",
        "      X[(row*i):(row*(i+1)), :] = Xcol\n",
        "\n",
        "    return X\n",
        "\n",
        "def index_num(u, v, k):\n",
        "    rowu = u.shape[0]\n",
        "    colu = u.shape[1]\n",
        "    ind_u = []\n",
        "    c = 0\n",
        "    for j in range(colu):\n",
        "      for i in range(rowu):\n",
        "        if u[i, j] == k :\n",
        "          temp_u = [i, j] \n",
        "          ind_u.append(temp_u)\n",
        "          c = c+1\n",
        "\n",
        "    uv = np.zeros(shape=[c])\n",
        "    for i in range(c):\n",
        "      temp_u = ind_u[i]\n",
        "      uv[i] = v[temp_u[0], temp_u[1]]\n",
        "\n",
        "    return uv\n",
        "\n",
        "def mon_cof(pmax, dimen):\n",
        "    nn = pmax+dimen-1\n",
        "    v = np.zeros(shape=[nn])\n",
        "    for i in range(nn):\n",
        "      v[i] = i+1\n",
        "\n",
        "    c = nchoosek(v, dimen-1)\n",
        "    m = c.shape[0]\n",
        "    t = np.ones(shape=[m, nn])  \n",
        "    m_in = np.zeros(shape=[m])\n",
        "    for i in range(m):\n",
        "      m_in[i] = i+1\n",
        "\n",
        "    m_in = np.transpose(m_in)\n",
        "    c_in = (c-1)*m\n",
        "    m_in = m_in.reshape(m_in.shape[0], 1)\n",
        "    t_in = repmat(m_in, 1, dimen-1)\n",
        "    t_in = t_in.reshape(c_in.shape[0], c_in.shape[1])\n",
        "    t_in = t_in + c_in \n",
        "    for i in range(t_in.shape[0]):\n",
        "      for j in range(t_in.shape[1]):\n",
        "        ind = t_in[i,j]\n",
        "        it = int(ind % m) -1\n",
        "        jt = int(np.ceil(ind/m)) -1\n",
        "        #print(it, jt)\n",
        "        t[it, jt] = 0\n",
        "\n",
        "    u = np.zeros(shape=[1, m])\n",
        "    u = np.concatenate((u, t.T))\n",
        "    u = np.concatenate((u, np.zeros(shape=[1, m])))\n",
        "    v = np.cumsum(u, axis = 0)\n",
        "    uv0 = index_num(u, v, 0)\n",
        "    uv = uv0.reshape(m, dimen+1)\n",
        "    uv = uv.T  \n",
        "    X = np.zeros(shape=[uv.shape[0]-1, m])  \n",
        "    for i in range(uv.shape[0]-1):\n",
        "      X[i, :] = uv[i+1, :] - uv[i, :]\n",
        "\n",
        "    X = X.T\n",
        "    return X\n",
        "\n",
        "def total_trunc(pmax, dimen):\n",
        "    for i in range(pmax, -1, -1):\n",
        "      if i == pmax:\n",
        "        idx = mon_cof(i, dimen)\n",
        "      else:\n",
        "        idx = np.concatenate((idx, mon_cof(i, dimen)))\n",
        "    idx = idx[::-1,:]\n",
        "\n",
        "    return idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqpU6_Rk7gGx"
      },
      "source": [
        "You can try generating the indices polynomial basis by executing the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7yja7jq7BA5"
      },
      "source": [
        "index = total_trunc(pmax=3, dimen=2)\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(index[:,0],index[:,1],s=100,c='b')\n",
        "plt.xlabel('$x_{1}$')\n",
        "plt.ylabel('$x_{2}$')\n",
        "plt.show()\n",
        "\n",
        "print(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_r_my4MM7_y"
      },
      "source": [
        "---\n",
        "## Polynomial Basis\n",
        "\n",
        "After we succesfully assign indices to all polynomial terms, the rest of the codes are straight from the theoritical definitions. Basically, we need to generate the regression matrix $\\boldsymbol{F}$ once we define the polynomial bases and the experimental design. Our code below (i.e., ```create_poly```) generates the regression matrix by taking polynomial indices, normalized sampling points, and polynomial type (i.e., monomial or Legendre). We also the Legendre polynomial as a recursive function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM-GkaTqNWpj"
      },
      "source": [
        "def create_poly(\n",
        "    Id_pol, Xnorm, poly_type\n",
        "):\n",
        "\n",
        "    Nsamp = Xnorm.shape[0]\n",
        "    dimen = Xnorm.shape[1]\n",
        "    nbase = Id_pol.shape[0]\n",
        "    F = np.zeros(shape=[Nsamp, nbase])\n",
        "\n",
        "    if poly_type in 'monomial':\n",
        "      for i in range(Nsamp):\n",
        "        for j in range(nbase):\n",
        "          temp = 1\n",
        "          for k in range(dimen):\n",
        "            temp = temp * Xnorm[i, k]**(Id_pol[j, k])\n",
        "          F[i, j] = temp\n",
        "      \n",
        "    elif poly_type in 'legendre':    \n",
        "      for i in range(Nsamp):\n",
        "        for j in range(nbase):\n",
        "          temp = 1\n",
        "          for k in range(dimen):\n",
        "            #temp = temp * spc.eval_legendre(Id_pol[j, k], Xnorm[i, k])\n",
        "            temp = temp * legendre_eval(Id_pol[j, k], Xnorm[i, k])         \n",
        "\n",
        "          F[i, j] = temp\n",
        "\n",
        "\n",
        "    return F   \n",
        "\n",
        "def legendre_eval(order, x):\n",
        "  if int(order) == 0:\n",
        "    fx = 1\n",
        "  elif int(order) == 1:\n",
        "    fx = x\n",
        "  elif order > 1:\n",
        "    fx = (2*order - 1)/order * x * legendre_eval(order-1, x) - (order - 1)/order * legendre_eval(order - 2, x)\n",
        "\n",
        "  return fx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_nkOpZb8W3H"
      },
      "source": [
        "The following cell is the subroutine for predicting the output at an arbitrary point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvkQ5MZK8Xux"
      },
      "source": [
        "def pred_PR(\n",
        "    Xpred, coef, Id_pol, poly_type\n",
        "):\n",
        "  Npred = Xpred.shape[0]\n",
        "  dimen = X.shape[1]\n",
        "  Xnpred = np.zeros(shape = [Npred, dimen])\n",
        "  \n",
        "  for i in range(dimen):\n",
        "      Xnpred[:, i] = (Xpred[:, i] - bounds[0, i])/(bounds[1, i] - bounds[0, i])\n",
        "\n",
        "  if poly_type in 'legendre':\n",
        "     for i in range(dimen):\n",
        "        Xnpred[:, i] = 2*(Xnpred[:, i] - 0.5)  \n",
        "\n",
        "  pred_pol = create_poly(Id_pol, Xnpred, poly_type)\n",
        "  y = pred_pol@coef\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khmq5TrTi4J2"
      },
      "source": [
        "We also define the following lines of code for calculating the LOOCV error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uO-SE9MSvpE"
      },
      "source": [
        "def LOOCV_PR(Id_pol, Xnorm, y, poly_type):\n",
        "  Nsamp = Xnorm.shape[0]\n",
        "  Ypred = np.zeros(shape=[Nsamp])\n",
        "  dimen = Xnorm.shape[1]\n",
        "\n",
        "  for i in range(Nsamp):\n",
        "    Xnpred = Xnorm[i, :].reshape(1, dimen)\n",
        "    Xtemp = np.delete(Xnorm, i, 0)\n",
        "    Ytemp = np.delete(y, i)\n",
        "\n",
        "    temp_poly = create_poly(Id_pol, Xtemp, poly_type)\n",
        "    term1 = temp_poly.T @ Ytemp\n",
        "    term2 = temp_poly.T @ temp_poly\n",
        "\n",
        "    coef = np.linalg.solve(term2, term1)\n",
        "    Ypred[i] = pred_PR_LOO(Xnpred, coef, Id_pol, poly_type)\n",
        "\n",
        "  rmse = np.sqrt(np.sum((Ypred - y)**2)/Nsamp)\n",
        "\n",
        "  return rmse\n",
        "\n",
        "def pred_PR_LOO(\n",
        "    Xnpred, coef, Id_pol, poly_type\n",
        "):\n",
        "  pred_pol = create_poly(Id_pol, Xnpred, poly_type)\n",
        "  y = pred_pol@coef\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDUAnBPENvDu"
      },
      "source": [
        "---\n",
        "## Main polynomial regression routine\n",
        "\n",
        "All that's left is to set up the main routine for the polynomial regression. The main routine is written in the following cell. Note that the code below implements automatic selection of polynomial order; you only need to define the maximum order (i.e., ```max_order```). Furthermore, you also need to define the type of polynomials, that is, either ```monomial``` or ```Legendre```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-DE8S-9Rc59"
      },
      "source": [
        "def main_PR(\n",
        "    X, y, bounds, max_order, poly_type = 'monomial'\n",
        "):\n",
        "\n",
        "  #Take all the required parameters\n",
        "  class result:\n",
        "    def __init__ (self):\n",
        "      self.coef = coef\n",
        "      self.residual = resd_all[id_min]\n",
        "      self.polynom = poly_all[id_min]\n",
        "      self.Id_pol = Id_all[id_min]\n",
        "      self.Id_pred = Id_pred\n",
        "      self.coef_all = coef_all\n",
        "      self.resd_all = resd_all\n",
        "      self.poly_all = poly_all\n",
        "      self.Id_all = Id_all\n",
        "      self.bounds = bounds\n",
        "      self.Xnorm = Xnorm\n",
        "      self.X = X\n",
        "      self.y = y\n",
        "      self.best_order = best_order\n",
        "      self.poly_type = poly_type\n",
        "      self.LOOCV_all = LOOCV_all\n",
        "      self.LOOCV = LOOCV_all[id_min]\n",
        "\n",
        "  # Normalize X into [-1, 1]\n",
        "  Nsamp = X.shape[0]\n",
        "  dimen = X.shape[1]\n",
        "  Xnorm = np.zeros(shape = [Nsamp, dimen])\n",
        "\n",
        "  for i in range(dimen):\n",
        "    Xnorm[:, i] = (X[:, i] - bounds[0, i])/(bounds[1, i] - bounds[0, i])\n",
        "\n",
        "  if poly_type in 'legendre':\n",
        "    for i in range(dimen):\n",
        "      Xnorm[:, i] = 2*(Xnorm[:, i] - 0.5)\n",
        "\n",
        "  # do loop for all orders\n",
        "  coef_all = []\n",
        "  resd_all = []\n",
        "  poly_all = []\n",
        "  Id_all = []\n",
        "  LOOCV_all = []\n",
        "\n",
        "  for i in range(max_order):\n",
        "    current_order = i+1\n",
        "    Id_pol = total_trunc(current_order, dimen)\n",
        "    nbase = Id_pol.shape[0]\n",
        "    if nbase > Nsamp:\n",
        "      raise NameError('Number of basis is higher than number of samples')\n",
        "\n",
        "    F = create_poly(Id_pol, Xnorm, poly_type)\n",
        "\n",
        "    #Find the optimum coefficient through LOOCV function\n",
        "\n",
        "    term1 = F.T @ y\n",
        "    term2 = F.T @ F\n",
        "    opt_coef = np.linalg.solve(term2, term1)\n",
        "\n",
        "    current_resd = residual_ls(opt_coef, y, F)\n",
        "    current_LOOCV = LOOCV_PR(Id_pol, Xnorm, y, poly_type)\n",
        "\n",
        "    coef_all.append(opt_coef)\n",
        "    resd_all.append(current_resd)\n",
        "    poly_all.append(F)\n",
        "    Id_all.append(Id_pol)\n",
        "    LOOCV_all.append(current_LOOCV)\n",
        "\n",
        "  #find model with lowest loocv\n",
        "  id_min = LOOCV_all.index(min(LOOCV_all))\n",
        "  coef = coef_all[id_min]\n",
        "  best_order = id_min + 1\n",
        "  Id_pred = total_trunc(best_order, dimen)\n",
        "\n",
        "  return result()\n",
        "\n",
        "def residual_ls(coef, y, M):\n",
        "  pred = M @ coef \n",
        "  resd = np.sum((y - pred)**2)\n",
        "\n",
        "  return resd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLs7XvfTJa5w"
      },
      "source": [
        "---\n",
        "## Example 1: Monomial Polynomial Regression\n",
        "\n",
        "The first test case for our polynomial regression model is a third order polynomial function with an extra sinusoidal term to make it more difficult, defined as\n",
        "\n",
        "$f(\\mathbf{x}) = x_{1}^2 + x_{1} + x_{2}^{3}-0.5\\text{sin}(x_{2}\\pi)$\n",
        "\n",
        "where $x_{i} \\in [-2,2]$.\n",
        "\n",
        " The following cell defines our test function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy2P48wgGUdb"
      },
      "source": [
        "def fun2D_polynom(X):\n",
        "  Nsamp = X.shape[0]\n",
        "  y = 1*X[:, 0]**2 + X[:, 0] + X[:, 1]**3 - 0.5*np.sin(X[:, 1]*np.pi)\n",
        "\n",
        "  y = y.reshape(Nsamp)\n",
        "\n",
        "  return y\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esmUpRMLJk7K"
      },
      "source": [
        "Now in this case, we are going to show the accuracy of polynomial regression model with a predefined maximum order. The main PR code will automatically select the best polynomial order that yields the lowest LOOCV error, which greatly reduces the overfitting risk. Please play around with the number of samples (```Nsamp```) and the maximum order (```max_order```) and observe how your prediction change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRStQ4TD2sWK"
      },
      "source": [
        "#2 dimensional example and 2 max order\n",
        "dimen = 2 # Input dimensionality\n",
        "Nsamp = 30# Number of sample\n",
        "max_order = 2 # Maximum polynomial order\n",
        "\n",
        "lb = np.array([-2, -2]) # Lower bounds\n",
        "ub = np.array([2, 2]) # Upper bounds\n",
        "bounds = np.array([lb, ub]) # Lower and upper bounds\n",
        "\n",
        "# Sampling points generation\n",
        "X = sampling_rlh(Nsamp, dimen) # Generate sampling points\n",
        "X = X*(bounds[1, :] - bounds[0, :]) + bounds[0, :] # Convert the bounds into the real scale\n",
        "y = fun2D_polynom(X) # evaluate the function\n",
        "\n",
        "# Construct PR model\n",
        "myPR = main_PR(X, y, bounds, max_order, poly_type = 'monomial')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djDndk9xEsCn"
      },
      "source": [
        "Please plot the results by executing the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ZX_4pu9g_Y"
      },
      "source": [
        "#@title Polynomial regression prediction on a two-dimensional function\n",
        "# For plotting\n",
        "numplot = 50\n",
        "Xpred = np.linspace(lb[0], ub[0], numplot)\n",
        "Ypred = np.linspace(lb[1], ub[1], numplot)\n",
        "\n",
        "Xplot, Yplot = np.meshgrid(Xpred, Ypred)\n",
        "Zplot = np.zeros(shape=[numplot, numplot])\n",
        "XYpred = np.zeros(shape=[numplot**2, dimen])\n",
        "c = 0\n",
        "for i in range(numplot):\n",
        "    for j in range(numplot):\n",
        "        XYpred[c, 0] = Xpred[i]\n",
        "        XYpred[c, 1] = Ypred[j]\n",
        "        Zplot[i, j] = fun2D_polynom(XYpred[c, :].reshape(1, dimen))\n",
        "        c = c+1\n",
        "\n",
        "ypred = pred_PR(XYpred, myPR.coef, myPR.Id_pred, myPR.poly_type)\n",
        "\n",
        "Zpred = np.zeros(shape = [numplot, numplot])\n",
        "c= 0\n",
        "for i in range(numplot):\n",
        "    for j in range(numplot):\n",
        "        Zpred[i, j] = ypred[c]\n",
        "        c = c+1    \n",
        "\n",
        "plt.figure(0)\n",
        "cf = plt.contourf(Xplot, Yplot, Zplot)\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.colorbar()\n",
        "plt.title(\"Real Surface\")\n",
        "\n",
        "plt.figure(1)\n",
        "cf = plt.contourf(Xplot, Yplot, Zpred)\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.colorbar()\n",
        "plt.title(\"Polynomial Regression Surface\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2crxtBCrE53O"
      },
      "source": [
        "You can check the polynomial order selected by the algorithm here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEFXO_CrE17-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8eae6c5-c933-4ff5-d9fe-239363324c38"
      },
      "source": [
        "print(\"The polynomial order that yields the best LOOCV error is \",myPR.best_order)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The polynomial order that yields the best LOOCV error is  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdiO_EGrCm_R"
      },
      "source": [
        "---\n",
        "## Example 2: Polynomial Chaos Expansion with Legendre Polynomial\n",
        "\n",
        "Please execute the following cell to test the polynomial regression with Legendre polynomials on the same two-dimensional function. We expect that there will be no significant difference between the predictive accuracy of PR and PCE for this problem. However, the differences may be more significant on other test functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ki8yj6lNYx6"
      },
      "source": [
        "#2 dimensional example\n",
        "dimen = 2 # Input dimensionality\n",
        "Nsamp = 80 # Number of sample\n",
        "max_order = 6 # Maximum polynomial order\n",
        "\n",
        "lb = np.array([-2, -2]) # Lower bounds\n",
        "ub = np.array([2, 2]) # Upper bounds \n",
        "bounds = np.array([lb, ub]) # Lower and upper bounds\n",
        "\n",
        "# Sampling points generation\n",
        "X = sampling_rlh(Nsamp, dimen) # Generate sampling points\n",
        "X = X*(bounds[1, :] - bounds[0, :]) + bounds[0, :] # Convert the bounds into the real scale\n",
        "y = fun2D_polynom(X) # Evaluate the function\n",
        "\n",
        "# Construct PR and PCE model\n",
        "myPR = main_PR(X, y, bounds, max_order, poly_type = 'legendre')\n",
        "myPCE = main_PR(X, y, bounds, max_order, poly_type = 'monomial')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxfF-MbfleWu"
      },
      "source": [
        "Please plot the results by executing the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TM0IINSHlewR"
      },
      "source": [
        "#@title PR and PCE prediction on a two-dimensional function\n",
        "\n",
        "numplot = 50 \n",
        "Xpred = np.linspace(lb[0], ub[0], numplot)\n",
        "Ypred = np.linspace(lb[1], ub[1], numplot)\n",
        "\n",
        "Xplot, Yplot = np.meshgrid(Xpred, Ypred)\n",
        "Zplot = np.zeros(shape=[numplot, numplot])\n",
        "XYpred = np.zeros(shape=[numplot**2, dimen])\n",
        "c = 0\n",
        "\n",
        "for i in range(numplot):\n",
        "  for j in range(numplot):\n",
        "    XYpred[c, 0] = Xpred[i]\n",
        "    XYpred[c, 1] = Ypred[j]\n",
        "    Zplot[i, j] = fun2D_polynom(XYpred[c, :].reshape(1, dimen))\n",
        "    c = c+1\n",
        "\n",
        "Zpred = np.zeros(shape = [numplot, numplot])\n",
        "Zpred2 = np.zeros(shape= [numplot, numplot])\n",
        "yval = np.zeros(shape = numplot**2)\n",
        "yval2 = np.zeros(shape = numplot**2)\n",
        "\n",
        "ypredPR = pred_PR(XYpred, myPR.coef, myPR.Id_pred, myPR.poly_type)\n",
        "ypredPCE = pred_PR(XYpred, myPCE.coef, myPCE.Id_pred, myPCE.poly_type)\n",
        "\n",
        "c= 0\n",
        "for i in range(numplot):\n",
        "  for j in range(numplot):\n",
        "    Zpred[i, j] = ypredPR[c]\n",
        "    Zpred2[i,j] = ypredPCE[c]\n",
        "    yval[c] = (Zpred[i,j] - Zplot[i,j])**2\n",
        "    yval2[c] = (Zpred2[i,j] - Zplot[i,j])**2\n",
        "    c = c+1    \n",
        "\n",
        "plt.figure(0)\n",
        "cf = plt.contourf(Xplot, Yplot, Zplot)\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.colorbar()\n",
        "plt.title(\"Real Surface\")\n",
        "\n",
        "plt.figure(1)\n",
        "cf = plt.contourf(Xplot, Yplot, Zpred)\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.colorbar()\n",
        "plt.title(\"Legendre Polynomial Regression Surface\")\n",
        "\n",
        "plt.figure(2)\n",
        "cf = plt.contourf(Xplot, Yplot, Zpred2)\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.colorbar()\n",
        "plt.title(\"Ordinary Polynomial Regression Surface\")\n",
        "\n",
        "print(\"Averaged RMSE:\")\n",
        "print(\"Legendre polynomial\")\n",
        "print(np.sum(yval)/(yval.shape[0]))\n",
        "print(\"Ordinary polynomial\")\n",
        "print(np.sum(yval2)/(yval.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1BLGUKNaOUr"
      },
      "source": [
        "---\n",
        "# Radial Basis Function\n",
        "\n",
        "Our second model is the Radial Basis Function (RBF) which is conceptually simpler than other kernel-based methods (e.g. support vector regression and Kriging) but also powerful. In contrast to PR that utilizes global basis functions, RBF employs more \"localized\" basis functions that are centered on the data points. \n",
        "\n",
        "The RBF model is expressed as follows\n",
        "\n",
        "$\\hat{f}(\\mathbf{x}) = \\boldsymbol{\\alpha}^{T}\\boldsymbol{\\varphi} = \\sum_{i=1}^{n} \\alpha_{i} \\varphi(\\|\\mathbf{x}-\\mathbf{c}^{(i)} \\|)$\n",
        "\n",
        "where $\\boldsymbol{\\alpha}$ is the vector of RBF coefficients, $\\boldsymbol{\\varphi}$ is the set of RBFs/kernel functions, and $\\mathbf{c}^{(i)}$ is the center of RBF (which is set as the data point $\\mathbf{x}^{(i))}$). In most RBF types, $\\boldsymbol{\\varphi}$ is a function of the shape parameter, that is, $\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\mathbf{x};\\theta)$. However, for the sake of simplicity, we eliminate the dependency of $\\boldsymbol{\\varphi}$ on $\\theta$ when writing down the basis function\n",
        "\n",
        "The coefficients $\\boldsymbol{\\alpha}$ are calculated by least squares. To be more exact, consider the following linear system\n",
        "\n",
        "\n",
        "\n",
        "> $\\mathbf{K}\\boldsymbol{\\alpha}=\\mathbf{y}$\n",
        "\n",
        "\n",
        "\n",
        "where $\\mathbf{K}$ is the Gram Matrix with its $(i,j)$ component is $\\mathbf{K}_{i,j} = \\varphi(\\|\\mathbf{x}^{(i)}-\\mathbf{x}^{(j)}\\|)$. The coefficients can then be simply computed by\n",
        "\n",
        "> $\\boldsymbol{\\alpha}=\\mathbf{K}^{-1}\\mathbf{y}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6soZV439Eo3I"
      },
      "source": [
        "## Kernel function\n",
        "One important building block of an RBF model is the **Kernel function**, or simply the basis function. In this tutorial, we will use three popular RBFs, namely: \n",
        "\n",
        "*   Gaussian\n",
        "> $\\text{exp}\\big(-\\frac{\\|r\\|^{2}}{\\theta} \\big)$\n",
        "\n",
        "*   Multiquadric\n",
        "> $\\big(1+\\frac{\\|r\\|^{2}}{\\theta} \\big)^{1/2}$\n",
        "*   Inverse multiquadric\n",
        "> $\\big(1+\\frac{\\|r\\|^{2}}{\\theta} \\big)^{-1/2}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyGUvvTTE0FX"
      },
      "source": [
        "def Kernel_RBF(\n",
        "    sigma, dist, kernel_type, shape_type\n",
        "):\n",
        "  if shape_type in 'Isotropic':\n",
        "\n",
        "    if kernel_type in 'Gaussian' :\n",
        "      kernel = np.exp((-1*dist**2)/sigma)\n",
        "    elif kernel_type in 'invmultiquadric' :\n",
        "      kernel = 1/(1+sigma*dist**2)\n",
        "    elif kernel_type in 'multiquadric' :\n",
        "      kernel = np.sqrt(1+sigma*dist**2)\n",
        "    else:\n",
        "      print(\"Kernel is not recognized!\")\n",
        "\n",
        "  elif shape_type in 'Anisotropic':\n",
        "\n",
        "    if kernel_type in 'Gaussian' :\n",
        "      kernel = np.exp(np.sum(-1*dist**2/sigma))\n",
        "    elif kernel_type in 'invmultiquadric' :\n",
        "      kernel = 1/np.sum(1+sigma*dist**2)\n",
        "    elif kernel_type in 'multiquadric' :\n",
        "      kernel = np.sqrt(np.sum(1+sigma*dist**2))\n",
        "    else:\n",
        "      print(\"Kernel is not recognized!\")\n",
        "\n",
        "  kernel = np.array([kernel])\n",
        "  return kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scUd3zCtE00A"
      },
      "source": [
        "## Calculation of RBF coefficients\n",
        "\n",
        "With a given $\\theta$ and kernel type, we can construct the Gram matrix $\\mathbf{K}$ based on the experimental design $\\mathbf{X}$ and the corresponding responses $\\mathbf{y}$. The next step is to calculate $\\boldsymbol{\\alpha}$ by solving $\\boldsymbol{\\alpha}=\\mathbf{K}^{-1}\\mathbf{y}$. The following cell does the job:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly8xEYZ2aS03"
      },
      "source": [
        "def model_RBF(\n",
        "    sigma, X, y, bounds, kernel_type, shape_type\n",
        "):\n",
        "\n",
        "  Nsamp = X.shape[0]\n",
        "  dimen = X.shape[1]\n",
        "\n",
        "  #normalize X\n",
        "  Xnorm = np.zeros(shape = [Nsamp, dimen])\n",
        "  for i in range(dimen):\n",
        "     Xnorm[:, i] = (X[:, i] - bounds[0, i])/(bounds[1, i] - bounds[0, i])\n",
        "\n",
        "  # Construct kernel matrix\n",
        "  Phi = np.zeros(shape=[Nsamp, Nsamp])\n",
        "\n",
        "  for i in range(Nsamp):\n",
        "     for j in range(Nsamp):\n",
        "       dist = np.sqrt(np.sum((Xnorm[i, :] - Xnorm[j, :])**2))\n",
        "       dist = np.array([dist])\n",
        "       Phi[i, j] = Kernel_RBF(sigma, dist, kernel_type, shape_type)\n",
        "\n",
        "  w = np.linalg.solve(Phi, y)\n",
        "\n",
        "  return w, Phi\n",
        "\n",
        "def check_posdef(Phi):\n",
        "  if np.all(np.linalg.eigvals(Phi) > 0):\n",
        "    check = 1\n",
        "  else:\n",
        "    check = 0\n",
        "\n",
        "  return check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUWGD1WKE9FG"
      },
      "source": [
        "## RBF prediction\n",
        "After the RBF model has been constructed (i.e., obtaining $\\boldsymbol{\\alpha}$), the calculation of RBF prediction is straightforward. The following cell builds the function for RBF prediction:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JduFJATiFA2s"
      },
      "source": [
        "def Pred_RBF(\n",
        "    xpred, sigma, X, w, bounds, kernel_type, shape_type\n",
        "):\n",
        "  dimen = X.shape[1]\n",
        "  Nsamp = X.shape[0]\n",
        "  Npred = xpred.shape[0]\n",
        "  norm_pred = np.zeros(shape=[Npred, dimen])\n",
        "  Xnorm = np.zeros(shape = [Nsamp, dimen])\n",
        "  for i in range(dimen):\n",
        "    norm_pred[:, i] = (xpred[:, i]- bounds[0, i])/(bounds[1, i]- bounds[0, i])\n",
        "    Xnorm[:, i] = (X[:, i] - bounds[0, i])/(bounds[1, i] - bounds[0, i])\n",
        "\n",
        "  \n",
        "  #contruct prediction rbf vector\n",
        "  Npred = xpred.shape[0]\n",
        "  \n",
        "  pred_phi = np.zeros(shape = [Npred, Nsamp])\n",
        "\n",
        "  for j in range(Npred):\n",
        "    for i in range(Nsamp):\n",
        "      dist = np.sqrt(np.sum((Xnorm[i, :] - norm_pred[j, :])**2))\n",
        "      dist = np.array([dist])\n",
        "      pred_phi[j, i] = Kernel_RBF(sigma, dist, kernel_type, shape_type)\n",
        "\n",
        "  ypred = np.zeros(shape=[Npred])\n",
        "  \n",
        "  for i in range(Npred):\n",
        "    ypred[i] = pred_phi[i, :] @ w\n",
        "\n",
        "  return ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7LD9bmlFK24"
      },
      "source": [
        "## LOOCV-based shape parameter optimization\n",
        "The shape parameter in RBF is usually optimized by minimizing the LOOCV error. There exists an analytical formulation to calculate the LOOCV error in an RBF model, reads as\n",
        "\n",
        "$\\hat{f}_{i}(\\mathbf{x}^{(i)})-y^{(i)} = \\frac{\\alpha_{i}}{\\mathbf{K}_{ii}}$,\n",
        "\n",
        "which can be used for any error metric (e.g. RMSE). However, because this tutorial is given for educational purpose, we will do it the hard way. That is, we will build $n$ extra RBF models to calculate the LOOCV error. However, in practive, note that we always use the analytical formulation to speed up the computation time.\n",
        "\n",
        "The following cell defines the subroutine for LOOCV calculation:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qxYb_ObFTJY"
      },
      "source": [
        "def LOOCV_RBF(\n",
        "    sigma, X, y, bounds, kernel_type, shape_type\n",
        "):\n",
        "\n",
        "  Nsamp = X.shape[0]\n",
        "  Ypred = np.zeros(shape=[Nsamp])\n",
        "\n",
        "  for i in range(Nsamp):\n",
        "    Xpred = X[i, :].reshape(1, dimen)\n",
        "    Xtemp = np.delete(X, i, 0)\n",
        "    Ytemp = np.delete(y, i)\n",
        "\n",
        "    wtemp, Phitemp = model_RBF(sigma, Xtemp, Ytemp, bounds, kernel_type, shape_type)\n",
        "    check = check_posdef(Phitemp)\n",
        "\n",
        "    if check == 0:\n",
        "      break\n",
        "\n",
        "    Ypred[i] = Pred_RBF(Xpred, sigma, Xtemp, wtemp, bounds, kernel_type, shape_type)\n",
        "  \n",
        "  if check == 1:\n",
        "    rmse = np.sqrt(np.sum((Ypred - y)**2)/Nsamp)\n",
        "    rmse = np.array([rmse])\n",
        "  elif check == 0:\n",
        "    rmse = 1e9\n",
        "    \n",
        "  return rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5H1Yg-CFWG0"
      },
      "source": [
        "## Main RBF routine\n",
        "Finally, let's combine all subroutines into an integrated code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQYqSgN-FboT"
      },
      "source": [
        "def main_RBF(\n",
        "    X, y, bounds, kernel_type = 'Gaussian', shape_type = 'Isotropic'\n",
        "):\n",
        "\n",
        "  class get_result:\n",
        "    def __init__ (self):\n",
        "      self.X = X\n",
        "      self.y = y\n",
        "      self.bounds = bounds\n",
        "      self.kernel_type = kernel_type\n",
        "      self.shape_type = shape_type\n",
        "      self.LOOCV = rmse\n",
        "      self.weight = w\n",
        "      self.sigma = sigma\n",
        "      self.Phi = Phi\n",
        "\n",
        "  dimen = X.shape[1]\n",
        "  if shape_type in 'Anisotropic':\n",
        "    sigma_init = np.ones(shape = [dimen])\n",
        "  else:\n",
        "    sigma_init = np.array([1])\n",
        "\n",
        "  params = (X, y, bounds, kernel_type, shape_type)\n",
        "  opt_param = optimize.minimize(LOOCV_RBF, sigma_init, args=params, method='BFGS')\n",
        "  sigma = opt_param.x\n",
        "  \n",
        "  rmse = LOOCV_RBF(sigma, X, y, bounds, kernel_type, shape_type)\n",
        "\n",
        "  w, Phi = model_RBF(sigma, X, y, bounds, kernel_type, shape_type)\n",
        "\n",
        "  return get_result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq_hbM17F2d-"
      },
      "source": [
        "## Example RBF1: Application to a one-dimensional function\n",
        "\n",
        "Our first test case is the Forrester's function, defined as\n",
        "\n",
        "$f(x)=(6x-2)^2\\text{sin}(12x-4)$\n",
        "\n",
        "In the cell below, you change the ```kernel_type``` and the number of samples (```Nsamp```) to see how it would affect the RBF prediction. The shape parameter is automatically optimized by using BFGS method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_japEejGAxj"
      },
      "source": [
        "# Create the sampling points\n",
        "dimen = 1 # Problem's dimensionality\n",
        "Nsamp = 10 # Number of samples, you can change this and see how it would affect the surrogate model\n",
        "ub = np.array([0]) # Upper bound\n",
        "lb = np.array([1]) # Lower bound\n",
        "bounds = np.array([lb, ub]) # Upper and lower bound\n",
        "\n",
        "X = np.linspace(0,1,num=Nsamp,endpoint=True).reshape(-1,1)\n",
        "y = (6*X-2)**2 *np.sin(12*X-4)\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "numplot = 50\n",
        "Xpred = np.linspace(0, 1, numplot).reshape(-1,1) # Prediction sites\n",
        "yreal = (6*Xpred-2)**2 *np.sin(12*Xpred-4) # True values at the prediction site\n",
        "\n",
        "# Construct the RBF model (notice that you can change the kernel function)\n",
        "myRBF = main_RBF(X, y, bounds,kernel_type = 'Gaussian') # Construct RBF\n",
        "ypred = Pred_RBF(Xpred, myRBF.sigma, myRBF.X, myRBF.weight, myRBF.bounds, myRBF.kernel_type, myRBF.shape_type) # Create prediction\n",
        "\n",
        "# Plotting\n",
        "plt.plot(Xpred, yreal, 'kx-') #\n",
        "plt.plot(Xpred, ypred, 'bd')\n",
        "plt.scatter(X,y,s=100,c='r')\n",
        "plt.xlabel('$x_{1}$')\n",
        "plt.ylabel('$y$')\n",
        "plt.legend(['True','Prediction','Sampling points'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-3UCbMFGMZq"
      },
      "source": [
        "---\n",
        "## Example RBF2: Application to a two-dimensional function\n",
        "The next problem is a two-dimensional function, reads as\n",
        "$f(\\mathbf{x})= x_{1}^{3}+x_{1}+x_{2}^{2}-x_{2}$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHB2HWfRG7jT"
      },
      "source": [
        "Let's try our RBF model on that function. The first step is to generate the sampling points. After that, we can build our two-dimensional RBF model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmDPU_1kG8aN"
      },
      "source": [
        "#2 dimensional example\n",
        "dimen = 2\n",
        "Nsamp = 30\n",
        "\n",
        "lb = np.array([-2, -2]) # Lower bounds\n",
        "ub = np.array([2, 2]) # Upper bounds\n",
        "bounds = np.array([lb, ub]) # Lower and upper bounds\n",
        "\n",
        "X = sampling_rlh(Nsamp, dimen) # Random latin hypercube sampling\n",
        "X = X*(bounds[1, :] - bounds[0, :]) + bounds[0, :] # Convert into the real scale\n",
        "y = fun2D_polynom(X) # Evaluate the function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmEejMVOGY10"
      },
      "source": [
        "After that, we can build our two-dimensional RBF model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCqckvuNqRDn"
      },
      "source": [
        "# Construct the RBF model \n",
        "myRBF2 = main_RBF(X, y, bounds, kernel_type = 'Gaussian') # Construct RBF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJGhtNLZqToX"
      },
      "source": [
        "Let's plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blDGWddy3ny9"
      },
      "source": [
        "#@title RBF prediction on a two-dimensional function\n",
        "# For plotting purpose\n",
        "numplot = 50\n",
        "Xpred = np.linspace(lb[0], ub[0], numplot)\n",
        "Ypred = np.linspace(lb[1], ub[1], numplot)\n",
        "\n",
        "Xplot, Yplot = np.meshgrid(Xpred, Ypred)\n",
        "Zplot = np.zeros(shape=[numplot, numplot])\n",
        "XYpred = np.zeros(shape=[numplot**2, dimen])\n",
        "\n",
        "c = 0\n",
        "for i in range(numplot):\n",
        "  for j in range(numplot):\n",
        "    XYpred[c, 0] = Xpred[i]\n",
        "    XYpred[c, 1] = Ypred[j]\n",
        "    Zplot[i, j] = fun2D_polynom(XYpred[c, :].reshape(1, dimen))\n",
        "    c = c+1\n",
        "\n",
        "# Plotting the RBF prediction\n",
        "ypred = Pred_RBF(XYpred, myRBF2.sigma, myRBF2.X, myRBF2.weight, myRBF2.bounds, myRBF2.kernel_type, myRBF2.shape_type) # Create prediction\n",
        "Zpred = np.zeros(shape = [numplot, numplot])\n",
        "\n",
        "c= 0\n",
        "for i in range(numplot):\n",
        "  for j in range(numplot):\n",
        "    Zpred[i, j] = ypred[c]\n",
        "    c = c+1\n",
        "\n",
        "plt.figure(0)\n",
        "cf = plt.contourf(Xplot, Yplot, Zplot)\n",
        "plt.colorbar()\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.title(\"Real Surface\")\n",
        "\n",
        "plt.figure(1)\n",
        "cf = plt.contourf(Xplot, Yplot, Zpred)\n",
        "plt.colorbar()\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.title(\"RBF Surface\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC5Z0fjXuBAD"
      },
      "source": [
        "---\n",
        "## Example RBF3: Application to the 10-dimensional Wing Weight Function\n",
        "The next example is more difficult due to its higher dimensionality. The function that we use is the so-called wing-weight function (see [here](https://www.sfu.ca/~ssurjano/wingweight.html) for more details). Because we use a traditional method for LOOCV calculation, the overall model building time is relatively time consuming. \n",
        "\n",
        "The following cell defines the wing-weight function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ_pkPrC3C8-"
      },
      "source": [
        "def wingweight(\n",
        "    X\n",
        "):\n",
        "\n",
        "  Nsamp = X.shape[0]\n",
        "  X[:, 3] = X[:, 3] * np.pi / 180\n",
        "  y = 0.036 * X[:, 0]**(0.758) * X[:, 1]**(0.0035) * (X[:, 2]/(np.cos(X[:, 3])**2))**(0.6) * X[:, 4]**(0.006) * X[:, 5]**(0.04) * (100*X[:, 6]/(np.cos(X[:,3])))**(-0.3) * X[:, 7] * X[:, 8] + X[:, 1] * X[:, 9]\n",
        "  y = y.reshape(Nsamp)\n",
        "\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsBdK0LBHFj5"
      },
      "source": [
        "The following cell builds the sampling points and also the evaluation data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOhdf-YguPNN"
      },
      "source": [
        "# 10 dimensional function\n",
        "dimen = 10\n",
        "\n",
        "lb = np.array([150, 220, 6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025]) # Lower bounds\n",
        "ub = np.array([200, 300, 10, 10, 45, 1, 0.18, 6, 2500, 0.08]) # Upper bounds\n",
        "bounds = np.array([lb, ub]) # Lower and upper bounds\n",
        "\n",
        "# for evaluation data set\n",
        "Nval = 1000 # Size of validation samples\n",
        "Xval = sampling_rlh(Nval, dimen) # Validation samples\n",
        "Xval = Xval*(bounds[1, :] - bounds[0, :]) + bounds[0, :] #Convert to real scale\n",
        "yval = wingweight(Xval) # Responses at validation samples\n",
        "\n",
        "# build the DoE\n",
        "Nsamp = 30  # Sample size\n",
        "\n",
        "X = sampling_rlh(Nsamp, dimen) # Sampling points\n",
        "X = X*(bounds[1, :] - bounds[0, :]) + bounds[0, :] #Convert to real scale\n",
        "y = wingweight(X) # Responses at sampling points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv_rY3_MHM60"
      },
      "source": [
        "Finally, the following cell builds our RBF model to approximate the 10-dimensional wing weight function (we have to wait for a while because we use a conventional LOOCV for shape parameter optimization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjokgjKfztsV"
      },
      "source": [
        "# RBF model building\n",
        "myRBF3 = main_RBF(X, y, bounds, kernel_type='Gaussian', shape_type='Isotropic')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgOwTiSiY64p"
      },
      "source": [
        "Since we cannot plot a ten-dimensional function, we can assess the accuracy by calculating the root-mean-squared-error on the validation dataset. We also normalize the error by the mean of validation samples so as to allow easier interpretation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRDISqqjZK2s"
      },
      "source": [
        "ypred = Pred_RBF(Xval, myRBF3.sigma, myRBF3.X, myRBF3.weight, myRBF3.bounds, myRBF3.kernel_type, myRBF3.shape_type) # Create prediction\n",
        "\n",
        "# calculate true error\n",
        "err = np.sqrt(np.sum((yval-ypred)**2)/Nval)\n",
        "true_err = err/np.mean(yval)\n",
        "print(\"Normalized RMSE:\")\n",
        "print(true_err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJvUIbaJZRI4"
      },
      "source": [
        "---\n",
        "# Application of Surrogate Models: Optimization of Branin function\n",
        "Reference: https://www.sfu.ca/~ssurjano/branin.html\n",
        "\n",
        "In the last part of this tutorial, we are going to implement PR and RBF to estimate the global optimum of a two-dimensional function. The function that we are going to optimize is the Branin function, which is defined as follows:\n",
        "\n",
        "$ f(\\mathbf{x}) = a (x_2 - bx_1^2 + cx_1 - r)^2 + s(1-t) cos(x_1) + s $\n",
        "\n",
        "where $a = 1, b = 5.1/(4\\pi^2), c = 5/\\pi, r = 6, s = 10, t = 1/(8\\pi) $.\n",
        "\n",
        "The branin function, or also known as Branin-Hoo function, has three global minima. The function is usually evaluated at $x_1 \\in [-5, 10], x_2 \\in [0, 15] $. The function is coded below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0bUfg9Rc1KB",
        "cellView": "form"
      },
      "source": [
        "#@title Branin function\n",
        "def Branin(\n",
        "    x, dimen = 2\n",
        "):\n",
        "  x_shape = x.shape\n",
        "  ns = x_shape[0]\n",
        "\n",
        "  f = np.zeros(shape=[ns]) \n",
        "  a = 1\n",
        "  b = 5.1/(4*np.pi**2)\n",
        "  c = 5/np.pi\n",
        "  r = 6\n",
        "  s = 10\n",
        "  t = 1/(8*np.pi)\n",
        "  for i in range(ns):  \n",
        "    f[i] = a*(x[i,1]-b*x[i,0]**2+c*x[i,0]-r)**2 + s*(1-t)*np.cos(x[i,0]) + s\n",
        " \n",
        "  return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RukJcheodzrj"
      },
      "source": [
        "Now, we are going to perform an optimization process aided by surrogate models. What we are going to do first is to create the surrogate model and then treat  the prediction as the objective function in the optimization process.\n",
        "\n",
        "First, let's set up the required dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASY-NHBaeCgo"
      },
      "source": [
        "dimen = 2 #Dimension of the problem\n",
        "\n",
        "lb = np.array([-5, 0]) # Lower bounds\n",
        "ub = np.array([10, 15]) # Upper bounds\n",
        "bounds = np.array([lb, ub]) # Lower and upper bounds\n",
        "\n",
        "# build the DoE\n",
        "Nsamp = 40  # Sample size\n",
        "\n",
        "X = sampling_rlh(Nsamp, dimen) # Sampling points\n",
        "X = X*(bounds[1, :] - bounds[0, :]) + bounds[0, :] #Convert to real scale\n",
        "y = Branin(X) # Responses at sampling points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y181pNaClbUm"
      },
      "source": [
        "And now we are going to make both polynomial regression model and radial basis function model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9p6_a7umPY2"
      },
      "source": [
        "# Construct the RBF model \n",
        "myRBF_opt = main_RBF(X, y, bounds, kernel_type = 'invmultiquadric', shape_type = 'Isotropic')\n",
        "myPCE_opt = main_PR(X, y, bounds, max_order = 4, poly_type = 'legendre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wKQOVVio9rD"
      },
      "source": [
        "What we need to do next is to define the objective function code. The following will do the job:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR48u7jsQlhC"
      },
      "source": [
        "## Objective function for RBF model\n",
        "def Obj_RBF(Xpred, output_RBF):\n",
        "  ypred = Pred_RBF(Xpred.reshape(1,-1), output_RBF.sigma, output_RBF.X, output_RBF.weight, output_RBF.bounds, output_RBF.kernel_type, output_RBF.shape_type)\n",
        "\n",
        "  return ypred\n",
        "\n",
        "def Obj_PCE(Xpred, output_PCE):\n",
        "  ypred = pred_PR(Xpred.reshape(1,-1), output_PCE.coef, output_PCE.Id_pred, output_PCE.poly_type)\n",
        "  return ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoBILpKfshQ1"
      },
      "source": [
        "Let's try to apply the two surrogate models for actual optimization process.\n",
        "Alright, all that's left is doing the actual optimization part. We are going to start at point $\\mathbf{x} = (0, 0)$ so that the point is not too far to the global minimum. We are going to use gradient-based method for the optimization process. Do note that using surrogate model does not prevent your optimization steps being stuck at local optimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA6GjhNvt0uT"
      },
      "source": [
        "## Optimization for RBF model\n",
        "x_init = np.array([0,0])\n",
        "\n",
        "opt_RBF = optimize.minimize(Obj_RBF, x_init, args = (myRBF_opt), method = 'BFGS')\n",
        "\n",
        "## Optimization for PCE model\n",
        "opt_PCE = optimize.minimize(Obj_PCE, x_init, args = (myPCE_opt), method = 'BFGS')\n",
        "\n",
        "print(\"Optimum X from RBF model:\")\n",
        "print(opt_RBF.x)\n",
        "print(\"Optimum value from RBF model:\")\n",
        "print(opt_RBF.fun)\n",
        "print(\"Optimum X from PCE model:\")\n",
        "print(opt_PCE.x)\n",
        "print(\"Optimum value from PCE model:\")\n",
        "print(opt_PCE.fun)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWrirsz7kv3G"
      },
      "source": [
        "According to references, the three global minima are located at $ \\mathbf{x}^{*} = (-\\pi, 12.275); (\\pi, 2.275); (9.42478, 2.475) $ and $f(\\mathbf{x}^*) = 0.397887 $. The nearest local optimum with our initial point is located at $(\\pi, 2.275) $. In practice, there are further methods to help tackling this problem, such as Expected Improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOwjCjSt56xD"
      },
      "source": [
        "# For plotting purpose\n",
        "numplot = 50\n",
        "Xpred = np.linspace(lb[0], ub[0], numplot)\n",
        "Ypred = np.linspace(lb[1], ub[1], numplot)\n",
        "\n",
        "Xplot, Yplot = np.meshgrid(Xpred, Ypred)\n",
        "Zplot = np.zeros(shape=[numplot, numplot])\n",
        "XYpred = np.zeros(shape=[numplot**2, dimen])\n",
        "c = 0\n",
        "\n",
        "for i in range(numplot):\n",
        "  for j in range(numplot):\n",
        "    XYpred[c, 0] = Xpred[i]\n",
        "    XYpred[c, 1] = Ypred[j]\n",
        "    Zplot[i, j] = Branin(XYpred[c, :].reshape(1, dimen))\n",
        "    c = c+1\n",
        "\n",
        "# Plotting the RBF prediction\n",
        "Zpred_RBF = np.zeros(shape = [numplot, numplot])\n",
        "Zpred_PCE = np.zeros(shape = [numplot, numplot])\n",
        "\n",
        "c= 0\n",
        "for i in range(numplot):\n",
        "  for j in range(numplot):\n",
        "    Zpred_RBF[i, j] = Obj_RBF(XYpred[c, :], myRBF_opt)\n",
        "    Zpred_PCE[i, j] = Obj_PCE(XYpred[c, :], myPCE_opt)\n",
        "    c = c+1\n",
        "\n",
        "lb_plot = -40\n",
        "ub_plot = 320\n",
        "levels_color = np.linspace(lb_plot, ub_plot, 9)\n",
        "\n",
        "plt.figure(0)\n",
        "cf = plt.contourf(Xplot, Yplot, Zplot, levels =levels_color)\n",
        "plt.colorbar()\n",
        "plt.clim(lb_plot, ub_plot)\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.title(\"Real Surface\")\n",
        "\n",
        "plt.figure(1)\n",
        "cf = plt.contourf(Xplot, Yplot, Zpred_RBF, levels = levels_color)\n",
        "plt.colorbar()\n",
        "plt.clim(lb_plot, ub_plot)\n",
        "plt.scatter(X[:,0],X[:,1],s=60,c='r')\n",
        "plt.title(\"RBF Surface\")\n",
        "\n",
        "plt.figure(2)\n",
        "cf = plt.contourf(Xplot, Yplot, Zpred_PCE, levels = levels_color)\n",
        "plt.colorbar()\n",
        "plt.clim(lb_plot, ub_plot)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=60, c='r')\n",
        "plt.title(\"PCE Surface\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdNmMu0wnOkb"
      },
      "source": [
        "While the optimum prediction is not really that accurate, the overall trend of the function is well-approximated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heBbwwZ0U-dD"
      },
      "source": [
        "Branin_RBF_err = np.zeros(shape=[numplot**2])\n",
        "Branin_PCE_err = np.zeros(shape = [numplot**2])\n",
        "c = 0\n",
        "for i in range(numplot):\n",
        "  for j in range(numplot):\n",
        "    Branin_RBF_err[c] = (Zplot[i,j] - Zpred_RBF[i,j])**2\n",
        "    Branin_PCE_err[c] = (Zplot[i,j] - Zpred_PCE[i,j])**2\n",
        "\n",
        "mean_val = np.sum(np.sum(Zplot))/numplot**2\n",
        "Branin_RBF_rmse = np.sqrt(np.sum(Branin_RBF_err)/numplot**2)/mean_val\n",
        "Branin_PCE_rmse = np.sqrt(np.sum(Branin_PCE_err)/numplot**2)/mean_val\n",
        "\n",
        "print(\"RMSE RBF: \")\n",
        "print(Branin_RBF_rmse)\n",
        "print(\"RMSE PCE: \")\n",
        "print(Branin_PCE_rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMGWTXFXvdVl"
      },
      "source": [
        "---\n",
        "# Afterword\n",
        "\n",
        "The main point of using surrogate model is to \"replace\" our functions of interest. Most researchs about surrogate modelling are motivated by the fact that some important functions are either expensive to evaluate or do not have necessary information such as derivatives. Besides optimization, surrogate modelling are also used for reliability analysis, sensitivity analysis, etc. Some surrogate models are more suited to handle spesific problems than others. Big data is the realm of deep neural network, small to medium engineering data is the domain of GPR/RBF, small to medium general regression data is the domain of gradient boosting, and UQ is the domain of PCE.\n"
      ]
    }
  ]
}