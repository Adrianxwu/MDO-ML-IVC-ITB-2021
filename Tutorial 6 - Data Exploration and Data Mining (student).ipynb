{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial 6 - Data Exploration and Data Mining (student).ipynb","provenance":[],"collapsed_sections":["o7SudslFHM1z"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yVNX2BSD8MO2"},"source":["# **TUTORIAL 6 - Data exploration and data mining**\n","\n","<img src=\"https://mdoml2021.ftmd.itb.ac.id/wp-content/uploads/2021/06/image.png\" width=\"15%\"><br>\n","<small>2021 ¬© MDOML IVC ITB</small>\n","\n","**Multidisciplinary Optimization and Machine Learning for Engineering Design International Virtual Course**\n","<br>\n","**19 July - 5 August 2021**\n","\n","Written by: Cahya Amalinadhi\n","<br>\n","Based on the lecture material by: Koji Shimoyama, Ph.D. (Tohoku University, Japan)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LMYDKrH3lSuN"},"source":["- Please run the following code first to import all required packages. \n","- In this tutorial, we will use `numpy`, `pandas`, `matplotlib`, and `seaborn`."]},{"cell_type":"code","metadata":{"id":"pTJrd_PKRBF6"},"source":["# Run this cell before starting\n","import numpy as np                  # for vector/matrix operation\n","import pandas as pd                 # for dataframe operation\n","\n","# For visualizing\n","# We use Matplotlib & Seaborn\n","import matplotlib.pyplot as plt\n","from matplotlib import patches\n","import seaborn as sns\n","%matplotlib inline\n","plt.style.use('seaborn')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7vTwd3X8QWq"},"source":["# **Descriptive Statistics**\n","---\n","What is the first step in data mining? There are various possible answers but analyzing the key statistics of the data seems to be a wise choice. The field of **descriptive statistics** aims to answer this by providing the user with several key statistics that describe the data. In data mining, it will be very useful to learn the general characteristics of the given data set, e.g. **central tendency**, **data dispersion**, and **maximum** or **minimum** of the data. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"o44q3gTC9tnr"},"source":["Let's begin with the **measures of central tendency**. The central tendency, as the name suggests, is a measure that shows the \"centerness\" of your data. Let's say that you have $n$ data collected into a vector $X = \\{x_{1},x_{2},\\ldots,x_{n}\\}$ and you want to calculate the central tendency of $X$. Several popular measures of central tendency that you can use include the following:\n","\n","\n","1.   Mean: denoted as $\\bar{X}$, where $\\bar{X}= (\\sum_{i=1}^{n}x_{i})/n$\n","2.   Median: By median, the data is sorted in ascending order. This sorted data is then split into the higher half and the lower half. The point where the data is split is called the median.\n","3.   Mode: The most frequent value in your data set.\n","\n","For example, if your data set is $X = \\{4,3,2,1,5,7,6,7,7\\}$ ($n=9$). Then,\n","\n","*   Your mean is $\\bar{X}=(4+3+2+1+5+7+6+7+7)/9 =4.6667$\n","*   Your median is 5. You got this by sorting your data first, i.e., $X_{sorted} = \\{1,2,3,4,\\boldsymbol{5},6,7,7,7\\}$. See that 5 is in the center.\n","*   Your mode is 7, as you can see that it appears 3 times.\n","\n","Besides the measures of central tendency, you typically want to know the **dispersion of your data with respect to your central tendency**. Some measures that you will typically use are:\n","\n","\n","*   **Standard deviation** ($\\sigma$), the most popular one, i.e., $\\sigma(X)=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(x_{i}-\\bar{X})}$\n","*  **Range**, probably the simplest one, where $\\text{Range}(X)=\\text{max}(X)-\\text{min}(X)$\n","*  **Interquartile range (IQR)$**, \n","the difference between the first and the third quartile (i.e.,  Q1 and Q3, respectively). See \"Quantile, percentile, quartile\" sections for more explanations about IQR.\n"]},{"cell_type":"markdown","metadata":{"id":"gHkWdlsdAqGU"},"source":["## **Measures of central tendency**"]},{"cell_type":"markdown","metadata":{"id":"0fJIi6YWAuNU"},"source":["We will use an imaginary stress data for the following examples, given as\n","\n","$X =\\{300,325,280,300,320,290,310,315,295,305\\}$,\n","\n","which will be written in the form of Numpy array.\n","\n","**Mean**\n","\n","- The most common way to measure the center of data is through mean.\n","\n","$$\n","\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}\n","$$\n","\n","> Fortunately, ```numpy``` provides a simple routine to calculate mean, namely. `np.mean(x)`. Try that below:"]},{"cell_type":"code","metadata":{"id":"-GGpbqLMBp4v"},"source":["# Create data. E.g stress of materials in MPa\n","stress = np.array([300, 325, 280, 300, 320, 290, 310, 315, 295, 305])\n","\n","# Find the mean of stress data\n","mean_stress = np.mean(stress)\n","print(f'Mean : {mean_stress} MPa')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_9KleWsC7IF"},"source":["**Median**\n","\n","- If the data distribution is highly skewed, it is better to measure the central of data with median. \n","- To find median, we have to sort the data in ascending order, then find the middle data according to the equation below:\n","$$\n","\\text{median} = \n","\\left\\{\\begin{matrix}\n","x_{(n+1)/2} & \\text{if } n \\text{ is odd} \\\\\n","(x_{(n)/2} + x_{(n+1)/2})/2 & \\text{if } n \\text{ is even} \\\\\n","\\end{matrix}\\right.\n","$$\n","\n","> The ```numpy``` syntax that we will use is : `np.median(x)`"]},{"cell_type":"code","metadata":{"id":"AG-Y3mDGEB6W"},"source":["# Find median\n","# 1. Sort the data in ascending order.\n","# 2. Find the middle data.\n","median_stress = np.median(stress)\n","print(f'Median : {median_stress} MPa')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ko-Vxsc1EWoF"},"source":["**Mode**\n","- The mode for the set of data is the value that occurs most frequently in the set. The value which appears the most in our data set is 300, so this is our mode for the stress data set.\n","\n","> We will use the ```stats``` package for that purpose: `stats.mode(x)`"]},{"cell_type":"code","metadata":{"id":"G79M58HpF7Ip"},"source":["# Import Scipy Library\n","from scipy import stats\n","\n","# Find mode\n","mode_stress = stats.mode(stress)[0][0]\n","print(f'Mode : {mode_stress} MPa')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kriuBtA1GpYp"},"source":["## **Measures of data dispersion**"]},{"cell_type":"markdown","metadata":{"id":"zTSInI5oGuB0"},"source":["**Standard Deviation and Variance**\n","\n","The most common measure of dispersion is the **standard deviation**, $\\sigma$, which can be simply computed as the square roof of the **variance** or $\\text{var}(X)$.\n","\n","\n","\n","$$ \n","\\text{std(X)} = \\sigma = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n}(x_i - \\text{mean(X)})^2}\n","$$\n","\n","$$ \n","\\text{var(X)} = \\sigma^2 = \\text{std(X)}^2\n","$$\n","\n","\n","> The Python syntax that we will use to calculate the standard deviation and the variance are, respectively,: `np.std(x)` and `np.var(x)`"]},{"cell_type":"code","metadata":{"id":"8EaCkB0dMLes"},"source":["# Calculate standard deviation\n","std_stress = np.std(stress)\n","print(f'Std. deviation : {std_stress:.2f} MPa')\n","print(f'Variance from standard deviation squared: {std_stress**2:.2f} MPa')        # square of std_stress\n","\n","# Calculate variance\n","var_stress = np.var(stress)\n","print(f'Variance from NumPy : {var_stress:.2f} MPa')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWt8OAJUNK0n"},"source":["# **Visualization Tools**\n","---\n","\n","\"A picture is worth a thousand words\", some people say. That might be true in many contexts! In data mining, visualization techniques play a very important role to aid users in extracting important knowledge from data. There are various visualization techniques but let's begin with some simple and commonly used techniques. In this tutorial, we will use:\n","1. Histogram\n","2. Boxplot \n","3. Correlation via scatter plot\n","\n","To be more specific, we will use ```seaborn``` which provides a lot of visualization techniques (which are eye pleasing too). We suggest you read more about ```seaborn``` after this tutorial and explore this wonderful tool for data visualization. As for now, let's use ```seaborn``` to visualize\n","our data set, namely ```machine_failures.csv```. We will also use ```pandas``` to read the CSV file and convert it into pandas dataframe.\n"]},{"cell_type":"code","metadata":{"id":"IO5TkOxzQ90O"},"source":["# Import data\n","data_machine_failures = pd.read_csv('machine_failures.csv')\n","data_machine_failures.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ujgfZo4yYxP"},"source":["Check the type of the data by typing ```type(data_machine_failures)```. You will see that we have successfully read our CSV data into a dataframe format."]},{"cell_type":"code","metadata":{"id":"mPfO-VvRyVmc"},"source":["type(data_machine_failures)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZoyDDCqOBIk"},"source":["## **Histogram**\n","\n","The very first type of plot that we will use is the histogram. It allows you to discover and show the frequency distribution of your data. By using a histogram, you can also inspect your data visually (e.g. checking the outliers or the skewness) or detect outliers or anomalies in your data. Histogram works by dividing your data into several bins, each bin has a range, and count the number of your data that fall into these independent bins. \n","\n","We will use a histogram to visualize the distribution of our machine failure data set. Histogram can provide clues to answer some of our questions, e.g., at which temperature that it is the most likely for the machine to fail? Seaborn provides us a tool to create histograms: `sns.histplot(data=, x=)`.\n","\n","Let's try that:"]},{"cell_type":"code","metadata":{"id":"J_iDyIjzSCIX"},"source":["# Line 4-6\n","# Create figures with 1 rows & 2 columns\n","# and set it height & width\n","fig, ax = plt.subplots(nrows=1, ncols=2)\n","fig.set_figheight(4)\n","fig.set_figwidth(10)\n","\n","# Line 10-11\n","# Extract the failure and no failure \n","data_fail = data_machine_failures[data_machine_failures[\"Failure\"] == \"Yes\"]\n","data_no_fail = data_machine_failures[data_machine_failures[\"Failure\"] == \"No\"]\n","\n","# Line 15-16\n","# Create histogram of failure data with Seaborn\n","sns.histplot(data=data_fail, x=\"Humidity\", ax=ax[0])\n","ax[0].set_title('Failure')\n","\n","# Line 20-21\n","# Create histogram of no failure data with seaborn\n","sns.histplot(data=data_no_fail, x=\"Humidity\", ax=ax[1])\n","ax[1].set_title('No Failure')\n","\n","plt.show()  # show the plot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oosyJpI8aUPV"},"source":["What did we find there? One important finding here is that the machine will likely experiences failure when the humidity is around 70%-75%.\n"]},{"cell_type":"markdown","metadata":{"id":"T1B22vfrY6YT"},"source":["## **Boxplot**\n","\n","Boxplot is very useful if you want to depict your data in terms of five numbers summary, namely, \n","1. median ($Q2$), \n","2. $Q1$, \n","3. $Q3$, \n","4. $Q1-1.5 IQR$ (minimum), and \n","5. $Q3+1.5 IQR$ (maximum). \n","\n","The line in the center is the median, the lower bound of the box is $Q1$, the upper bound of the box is $Q2$, the whisker on the bottom is $Q1-1.5 IQR$, and the whisker on the top is $Q3+1.5 IQR$. Remember that you can also change the orientation of the boxplot from vertical to horizontal. The figure below depicts the anatomy of a boxplot so you can understand the meaning of the box and the whisker in a boxplot.\n","\n","<center>\n","<img src=\"https://i1.wp.com/quantgirl.blog/wp-content/uploads/2019/02/box_anatomy-3-e1549228924225.png\" />\n","</center>\n","\n","[Figure source](https://i1.wp.com/quantgirl.blog/wp-content/uploads/2019/02/box_anatomy-3-e1549228924225.png)\n","\n","The Python syntax that we will use is: `sns.catplot(data=, x=, y=, kind=)`\n","\n","The cell below visualizes again our humidity information with a boxplot."]},{"cell_type":"code","metadata":{"id":"hw670U5kZJhC"},"source":["# We use catplot (categorical plot) to plot based on category/class -> Failure & No Failure\n","# Then, we use a boxplot (kind=\"box\") to plot box plot.\n","sns.catplot(data=data_machine_failures, x=\"Failure\", y=\"Humidity\", kind=\"box\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7MN3pfn0t-6"},"source":["Let's try it again one more time but we will use the temperature data instead of the humidity: "]},{"cell_type":"code","metadata":{"id":"ILHFiWBrAjbY"},"source":["# We use catplot (categorical plot) to plot based on category/class -> Failure & No Failure\n","# Then, we use a boxplot (kind=\"box\") to plot box plot.\n","sns.catplot(data=data_machine_failures, x=\"Failure\", y=\"Temperature\", kind=\"box\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oeB8QKU1auNx"},"source":["So what did we just find here? You might have your interpretations, but the followings are clear:\n","\n","*   There are several notable outliers in the no-fail data.\n","*   In general, it is more likely for the machine to experience failure at high temperatures (which makes sense, right?)\n","* In the context of classification, the outliers might be harmful when we want to create a machine learning model. Detecting and treating outliers can be very useful to improve the predictive accuracy of our classification model.\n"]},{"cell_type":"code","metadata":{"id":"aPZGubj5hUD9"},"source":["# Clean the data from the boxplots above\n","\n","# Delete the data of failures with humidity >= 100\n","data_cleaned = data_machine_failures[data_machine_failures[\"Humidity\"] < 100]\n","\n","# Delete the data of failures with Temperature <= 60\n","data_cleaned = data_cleaned[data_cleaned[\"Temperature\"] > 60]\n","\n","# Pick the failure data\n","data_cleaned = data_cleaned[data_cleaned[\"Failure\"] == \"Yes\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S8FUv-hwew7B"},"source":["## **Correlation**\n","When analyzing two different data sets, you might have an interest in investigating the relationship between the two data sets. You have questions such as: (1) If I increase $X$, will $Y$ decrease?, (2) Does the change in $X$ affects $Y$?, (3) if there is indeed a relationship, how strong is the relationship? etc.\n","\n","One way to answer such questions is to use **Pearson correlation coefficient** which measures the linear relationship between two datasets. The Pearson correlation coefficient is calculated as\n","\n","$$\n","\\rho_{X,Y} = \\text{corr}(X,Y)=\\frac{\\text{cov}(X,Y)}{\\sigma_{X}\\sigma_{Y}}=\\frac{\\mathbb{E}[(X-\\mu_{X})(Y-\\mu_{Y})]}{\\sigma_{X}\\sigma_{Y}}\n","$$\n","\n","where $\\mathbb{E}$ is the expectation and $\\text{cov}$ is the covariance.\n","\n","In practice, $\\rho_{X,Y}$ is estimated from your sample as\n","\n","$$\n","\\rho_{X,Y} = \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}} \\sqrt{\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}}}\n","$$\n","\n","The value of $\\rho_{X,Y}$ varies between -1 and +1. In this regard, $\\rho_{X,Y}=1$ indicates an exact linear positive relationship (i.e., $X$ increases if $Y$ increases), while $\\rho_{X,Y}=-1$ indicates an exact linear negative relationship (i.e., $X$ increases if $Y$ decreases). If $\\rho_{X,Y}=0$, or very close to zero, this implies no correlation.\n","\n","First, let's use `sns.scatterplot(data=, x=, y=)` to create a scatter plot to visualize the relationship between humidity and temperature, together with the best linear regression line (the confidence interval is also shown):"]},{"cell_type":"code","metadata":{"id":"76g34TVYdviK"},"source":["sns.scatterplot(data=data_fail, x=\"Humidity\", y=\"Temperature\")   # Create 2D scatter plot\n","sns.regplot(data=data_fail, x=\"Humidity\", y=\"Temperature\", \n","            color=\"red\", scatter=False)                          # Create regression line\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTggsSrjn64Y"},"source":["We can clearly see that the temperature and humidity are negatively correlated, i.e., the humidity increase when the temperature drops.\n","\n","How about for cases with multiple variables? We can use a correlation matrix for that purpose. We need to calculate the correlation coefficient between each pair of variables first by using `np.corrcoef` and then use `sns.heatmap` to visualize the correlation matrix:"]},{"cell_type":"code","metadata":{"id":"oVCkeODRj7Of"},"source":["# Drop categorical data first\n","index_to_drop_1 = data_fail.columns[-8:]\n","index_to_drop_2 = ['Date', 'Operator']\n","\n","# Create new data\n","data_fail_cleaned = data_fail.drop(index_to_drop_1, axis=1)\n","data_fail_cleaned = data_fail_cleaned.drop(index_to_drop_2, axis=1)\n","\n","# Create correlation coefficients\n","corr = np.corrcoef(data_fail_cleaned, rowvar=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5dw1_mJ8II1"},"source":["The cell below performs the visualization:"]},{"cell_type":"code","metadata":{"id":"PYRIyB3snB43"},"source":["# Convert corr into pandas dataframe\n","corr_pd = pd.DataFrame(data=corr, \n","                       columns=data_fail_cleaned.columns,\n","                       index=data_fail_cleaned.columns)\n","\n","# Plot\n","fig, ax = plt.subplots(nrows=1, ncols=1)\n","fig.set_figheight(8)\n","fig.set_figwidth(15)\n","\n","sns.heatmap(corr_pd,\n","            annot=True, fmt=\".2f\", ax=ax)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"na9zIlpj8Rg_"},"source":["- The only thing that looks clear here is that the temperature and the humidity are negatively correlated. \n","- On the other hand, the correlations between the other pairs of variables are relatively low. \n","- We then conclude that temperature and humidity are the most relevant variables in our current data set.\n","\n","One side note, remember that correlation does not indicate causation. The following cartoon illustrates it well:\n","\n","![https://amplitude.com/](https://images.ctfassets.net/vrkkgjbn4fsk/6zStaOhyU0II1ktEbqsFUN/1504e23f790afdda052f3516b8bc1089/correlation-vs-causation.png)\n","\n","[Figure Source](https://images.ctfassets.net/vrkkgjbn4fsk/6zStaOhyU0II1ktEbqsFUN/1504e23f790afdda052f3516b8bc1089/correlation-vs-causation.png)"]},{"cell_type":"markdown","metadata":{"id":"8W-SnJW5q_fQ"},"source":["# **High-dimensional Data Mining**\n","---\n","A correlation matrix is just one tool to visualize and analyze our high-dimensional data set. In fact, we have more tools in our arsenal that we can deploy to uncover other important knowledge from our data. This tutorial will talk about three methods, namely:\n","\n","\n","*   Parallel Coordinates Plot (PCP)\n","*   K-means clustering\n","*   Self-organizing map\n"]},{"cell_type":"markdown","metadata":{"id":"wtP2nIy2mfIR"},"source":["## **Parallel Coordinates Plot (PCP)**\n","PCP is a simple but very useful method to visualize relationships between multivariate numerical data. PCP does this job by mapping and visualizing the connection between all features in data with lines. It would be much easier for us to understand what PCP is if we apply it in real data. We will use the famous iris data set for that purpose (not an engineering data set, but we use it due to its simplicity). The data is not high-dimensional (i.e., only four features/variables) but it is a good test problem for us to better grasp the concept of PCP.\n","\n","The iris data set is frequently used in various contexts, which includes classification and data visualization. There are four features in the iris data set, namely, (1) sepal width, (2) sepal length, (3) petal length, and (4) petal width. Furthermore, there are three classes for this data set, namely, (1) iris setosa, (2) iris versicolor, and (3) iris virginica.\n","\n","Our goal is now to see the relation between features and classes by using PCP.\n","\n","![](https://bishwamittra.github.io/images/imli/iris_dataset.png)\n","\n","[Figure Source](https://bishwamittra.github.io/images/imli/iris_dataset.png)"]},{"cell_type":"markdown","metadata":{"id":"Ehs9bvJ29a4R"},"source":["First, we need to load the dataset from ```sklearn```."]},{"cell_type":"code","metadata":{"id":"T8pKxSl2do-M"},"source":["# Import library\n","from sklearn.datasets import load_iris              # load Iris dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ilxsx5HACtDw"},"source":["The following cell performs some operators to create a dataframe of the iris data set:"]},{"cell_type":"code","metadata":{"id":"FNW9T6-pqUtC"},"source":["# Load Data\n","# The script below is to make sure we generate proper dataset\n","iris = load_iris()\n","iris_data = np.hstack((iris.data, iris.target.reshape(-1,1)))\n","iris_df = pd.DataFrame(data=iris_data, \n","                       columns=iris.feature_names+[\"flower type\"])\n","iris_df[\"flower type\"].replace({0.0: \"setosa\", \n","                                1.0: \"versicolor\", \n","                                2.0: \"virginica\"}, inplace=True)\n","iris_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NaGqxifo25Tt"},"source":["Notice that we can use multiple scatter plots to visualize the hidden relationships in our iris data set. Let's try that by visualizing sepal variation and petal variation:"]},{"cell_type":"code","metadata":{"id":"6RFloVar3FVA"},"source":["fig, ax = plt.subplots(nrows=1, ncols=2)\n","fig.set_figheight(6)\n","fig.set_figwidth(18)\n","\n","# The scatter plot for sepal variation\n","sns.scatterplot(data=iris_df, x=\"sepal length (cm)\", y=\"sepal width (cm)\", \n","                hue=\"flower type\", ax=ax[0])\n","ax[0].set_title(\"Sepal Variation\")\n","\n","# The scatter plot for petal variation\n","sns.scatterplot(data=iris_df, x=\"petal length (cm)\", y=\"petal width (cm)\", \n","                hue=\"flower type\", ax=ax[1])\n","ax[1].set_title(\"Petal Variation\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NT3HDKRf6Az3"},"source":["The figures surely look nice. However, how about if we have let's say 21-dimensional data? Surely you don't want to see too many scatter plots and they might be too cluttered for us to discover the hidden relationships.\n","\n","Luckily for us, PCP does that job by compressing all information into a two-dimensional plane so that it is easier for us to comprehend the data. Pandas provides `pd.plotting.parallel_coordinates()` to easily perform PCP. \n"]},{"cell_type":"code","metadata":{"id":"8MKJjbyBqmYP"},"source":["fig = plt.figure(figsize=(10,6))\n","pd.plotting.parallel_coordinates(frame=iris_df, class_column='flower type',\n","                                 cols=[\"petal width (cm)\", \"sepal width (cm)\", \n","                                       \"petal length (cm)\", \"sepal length (cm)\"],\n","                                 colormap='jet', alpha=0.5)\n","plt.title(\"IRIS Flowers Parallel Coordinates Plot\")\n","plt.grid()\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ToAcA-bDsf2"},"source":["I think it is easy for us to get the main idea. We can even see that all the three species are distinguishable just by observing the differences in the four features! Here are some trends that we observe:\n","\n","* `setosa` has the smallest petal width, while `virginica` has the largest petal width.\n","* The three species are roughly similar in terms of sepal width, although it can still be seen that `setosa` individuals generally have longer sepal width. \n","* For the petal length, `virginica` is the largest while `setosa` is the smallest.\n","* `setosa`'s sepal length is the smallest compared to the others\n","* Etc\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ywelamJv2G3b"},"source":["**PCP with normalized data**\n","\n","Sometimes, we want to normalize the data set before we construct a PCP plot. Normalization is very useful if the features greatly differ in terms of magnitude. We will use the ```MinMaxScaler``` from ```sklearn``` to scale all the data into $[0,1]$ based on the maximum and minimum of each feature.\n"]},{"cell_type":"code","metadata":{"id":"FMaRfMJA1WQ3"},"source":["# Load scaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Normalize the data\n","# Using min-max scaler to range [0, 1]\n","iris_scaled = MinMaxScaler().fit_transform(iris.data)\n","iris_data_scaled = np.hstack((iris_scaled.data, iris.target.reshape(-1,1)))\n","iris_df_scaled = pd.DataFrame(data=iris_data_scaled, \n","                              columns=iris.feature_names+[\"flower type\"])\n","iris_df_scaled[\"flower type\"].replace({0.0: \"setosa\", \n","                                       1.0: \"versicolor\", \n","                                       2.0: \"virginica\"}, inplace=True)\n","iris_df_scaled.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PmKaE6KUFLVN"},"source":["The following cell does the job:"]},{"cell_type":"code","metadata":{"id":"gqqTVJ2E2Jkh"},"source":["fig = plt.figure(figsize=(10,6))\n","pd.plotting.parallel_coordinates(frame=iris_df_scaled, class_column='flower type',\n","                                 cols=[\"petal width (cm)\", \"sepal width (cm)\", \n","                                       \"petal length (cm)\", \"sepal length (cm)\"],\n","                                 colormap='jet', alpha=0.5)\n","plt.title(\"IRIS Flowers Parallel Coordinates Plot [SCALED]\")\n","plt.grid()\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I8W1s3l82TmW"},"source":["Although the has been normalized, notice that the plot is still cluttered so we need to rearrange it somehow:"]},{"cell_type":"code","metadata":{"id":"z-RhM6Q12bIo"},"source":["fig = plt.figure(figsize=(10,6))\n","pd.plotting.parallel_coordinates(frame=iris_df_scaled, class_column='flower type',\n","                                 cols=[\"petal length (cm)\", \"petal width (cm)\",\n","                                       \"sepal length (cm)\", \"sepal width (cm)\"],\n","                                 colormap='jet', alpha=0.5)\n","plt.title(\"IRIS Flowers Parallel Coordinates Plot [SCALED & RE-ORDERED]\")\n","plt.grid()\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r2-BWuLyG_7T"},"source":["Clearly looks better! It is now even easier to see the differences between the three species (e.g. they differ primarily in petal length and petal width)."]},{"cell_type":"markdown","metadata":{"id":"DbutricJOVVQ"},"source":["## **$k$-Means Clustering**\n","\n","Clustering is a very important unsupervised machine learning task and it has also been used to aid the design optimization process. Especially in multi-objective optimization, some researchers use clustering to select representative solutions from the set of Pareto optimal solutions. This tutorial uses **k-means clustering** to demonstrate the working mechanism of clustering. In essence, k-means clustering partition $n$ observations into $k$ cluster. The class that a single observation will belong to is determined based on the distance of that observation to the cluster centers (i.e. to the center with the nearest distance).\n"]},{"cell_type":"markdown","metadata":{"id":"AqbDcK8AO3J7"},"source":["---\n","K-means clustering involves some randomness in the determination of the cluster. For example, the final result depends on the initial random clustering.\n","**Algorithms**. There are several initialization methods but all of them follow the same pseudocode:\n","\n","```\n","1. Input data and number of clusters\n","2\n","3. REPEAT\n","4.     A: Assign each data to the closest cluster (find the closest distance between data and centroids)\n","5.     B: Calculate new centroids\n","6. UNTIL centroids are not change\n","```\n","\n","K-means clustering chooses the centroids so as to minimize the inertia, or the within-cluster sum of squares (see this [explanation](https://scikit-learn.org/stable/modules/clustering.html#k-means) for more details).\n"]},{"cell_type":"markdown","metadata":{"id":"lTsjabWwSnAL"},"source":["We will generate a random unlabelled data set by using ```make_blobs``` from ```sklearn```. We will then use ```Kmeans``` from ```sklearn``` to perform our clustering task. The following cell generates 2000 random observations, in which each observation is generated from a single blob with a specified center and standard deviation:\n"]},{"cell_type":"code","metadata":{"id":"hLgef1vqOGxw"},"source":["# Import data\n","from sklearn.datasets.samples_generator import make_blobs\n","X, y = make_blobs(n_samples=2000, centers=7, cluster_std=0.7, random_state=0)\n","\n","# Plot\n","plt.scatter(X[:,0], X[:,1], s=10)\n","plt.xlabel('design variable 1')\n","plt.ylabel('design variable 2')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HvV_54uIJhfR"},"source":["You can clearly see that there are seven blobs. However, each observation is still unlabelled so let's give them labels based on our clustering algorithm. Notice again that $k$ in k-means clustering indicates the number of desired clusters. That is, $k$ is a user-defined parameter. Let's play around by setting $k=3$ first:"]},{"cell_type":"code","metadata":{"id":"sMcmg33gR0QH"},"source":["# Clustering\n","from sklearn.cluster import KMeans\n","\n","# Define number of cluster (step 1)\n","no_clusters = 3\n","\n","# Start fitting (step 3-6)\n","cluster_model = KMeans(n_clusters=no_clusters)      # create object called by cluster_model\n","cluster_model.fit(X)                                # fit the data (X) \n","\n","# Clustering results\n","y_pred = cluster_model.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DjHd2cW8THTh"},"source":["The following cell visualizes the locations of the centers (see the yellow dots). The plot also visualizes the label of each observation based on the proximity to the center. "]},{"cell_type":"code","metadata":{"id":"3WGIEk5zTP4X"},"source":["# Find centroids\n","centroids = cluster_model.cluster_centers_\n","\n","# Plot\n","plt.scatter(X[:,0], X[:,1], c=y_pred, edgecolors='black', cmap='jet', s=10)\n","plt.scatter(centroids[:,0], centroids[:,1], s=200, c='yellow', alpha=0.8)\n","plt.title('Clustering prediction')\n","plt.xlabel('design variable 1')\n","plt.ylabel('design variable 2')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wOzC7Zf_Vj2U"},"source":["**Finding the best $k$**\n","Choosing the right value of $k$ is not trivial, especially if the number of features/input variable is higher than two. The natural choice is to find the value of $K$ so that the inertia is minimized. The inertia value can be extracted as follows:"]},{"cell_type":"code","metadata":{"id":"-zUdoSzYV89H"},"source":["# From previous cases (no_clusters = 3)\n","clust_obj = cluster_model.inertia_\n","print(clust_obj)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zeSVCHt7c9E4"},"source":["What would happen if we increase $K$? Let's try it again but with $K=8$:"]},{"cell_type":"code","metadata":{"id":"GqmPzlZDWMHl"},"source":["# We increase the number of clusters, let say no_cluster = 8\n","no_clusters = 8\n","cluster_model = KMeans(n_clusters=no_clusters)      \n","cluster_model.fit(X)                                \n","clust_obj = cluster_model.inertia_\n","print(clust_obj)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ab6JS8EadEXL"},"source":["The idea is to keep increasing the $K$ until we think that it is sufficient. The word \"sufficient\" here is important because it is highly likely that the inertia keeps decreasing. We do not want too much clusters, or later analysis would become too complex. Therefore, we need to balance the value of the inertia and also the number of cluster. \n","\n","The following cell performs an experiment that varies $K$ from 2 to 20. See how the inertia decreases until the gradient of the curve becomes small; that's the sign when you have to stop increasing the number of cluster.\n"]},{"cell_type":"code","metadata":{"id":"A4h-sB6fWc5i"},"source":["# We can plot this no_cluster = [2, ..., 20]\n","no_clusters_list = np.arange(2,21,1)\n","\n","clust_obj_list = []\n","for i in no_clusters_list:\n","    no_clusters = i\n","    cluster_model = KMeans(n_clusters=no_clusters)      \n","    cluster_model.fit(X)                                \n","    clust_obj = cluster_model.inertia_\n","\n","    clust_obj_list.append(clust_obj)\n","\n","# plot the inertia\n","plt.plot(no_clusters_list, clust_obj_list)\n","plt.scatter(no_clusters_list, clust_obj_list, c='red')\n","plt.title('The elbow curve')\n","plt.xticks(no_clusters_list)\n","plt.xlabel('number of clusters')\n","plt.ylabel('cluster objective (inertia)')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUMUq3k5EI59"},"source":["We can see that $K=7$ seems to be the most optimal value for $K$. Increasing $K$ to 8 seems to decrease the inertia a little. On the other hand, decreasing $K$ to 6 significantly increases the value of the inertia.\n","\n","As a side note, the choice of $K$ totally depends on the context. Sometimes the value of $K$ is determined a priori and does not need to be optimized."]},{"cell_type":"code","metadata":{"id":"kTFMJZkEZfgE"},"source":["# Best clusters\n","no_clusters = 7\n","cluster_model = KMeans(n_clusters=no_clusters)      \n","cluster_model.fit(X)  \n","y_pred = cluster_model.predict(X) \n","\n","# Find centroids\n","centroids = cluster_model.cluster_centers_\n","\n","# Plot\n","plt.scatter(X[:,0], X[:,1], c=y_pred, edgecolors='black', cmap='jet', s=10)\n","plt.scatter(centroids[:,0], centroids[:,1], s=200, c='yellow', alpha=0.8)\n","plt.title('Clustering prediction')\n","plt.xlabel('design variable 1')\n","plt.ylabel('design variable 2')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mp8C5E97wN7d"},"source":["## **Self-Oganizing Maps (SOM)**\n","The final technique that we will use is the SOM. SOM is highly useful in the design exploration and optimization phase to visualize, for example, high-dimensional. We can also use SOM to visualize any high-dimensional data set so as to extract important knowledge from the data. SOM is essentially an unsupervised neural network that aims to find a low-dimensional representation of a high-dimensional data set while preserving the topological structure of data. The basic idea goes like the following:\n","\n","\n","*   SOM clusters similar data points in a two-dimensional grid. \n","*   Each data point is mapped to one node in this two-dimensional grid.\n","*   Most importantly, data points that are similar according to their topological features are mapped to the same node.\n","*   Nearby data points are also ended up being mapped nearby.\n"]},{"cell_type":"markdown","metadata":{"id":"x2bGmjIrBBvI"},"source":["---\n","The basic algorithm for the SOM method is as follows:\n","\n","- We initially create a map with some nodes on it.\n","- Each node in map is associated with a \"weight\" vector, which is the position of the node in the input space.\n","- While the nodes in the map stay fixed, we \"train\" the weight vectors toward the input data (by reducing the distance between weight vectors and the data)\n","- The figure below shows a visualization of how the nodes change their location towards the data.\n","- On the other hand, what we see in a SOM map is a fixed nodes.\n","- Thus, nodes with similar magnitude have similar characteristics.\n","\n","<center>\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/TrainSOM.gif/220px-TrainSOM.gif\" />\n","</center>\n","\n","[Figure Source](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/TrainSOM.gif/220px-TrainSOM.gif)\n","\n","- Select a data and find the node closest to the data. We also called it the Best Matching Unit (BMU)\n","- Move the node and **its neighbors** toward the data. To move the node means to update its weight.\n","- The neighbors are also updated. The neighbors are nodes that are within a defined diameter sigma.\n","- To prevent unnecesarry movement, we set initial *learning rate* as a step size.\n","- The region (sigma) and the learning rate will decay along with the iteration time.\n"]},{"cell_type":"markdown","metadata":{"id":"o7SudslFHM1z"},"source":["### **SOM from Scratch**\n","We can code SOM from stratch and fortunately it is doable to do s. The SOM are coded in the following cells. We will the use this SOM to organize a simple data set based on random colors (i.e., 1000 color data):"]},{"cell_type":"code","metadata":{"id":"DPZEZN7Ytlcu"},"source":["n_dim = 3           # dimension of feature (in color, it's 3 dim)\n","n_data = 1000       # number of data\n","\n","# generate data\n","data_color = np.random.randint(0, 256, (n_data, n_dim))\n","data_color_norm = data_color/255"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJGwQpYfu2Hh"},"source":["# Create SOM map\n","# we set row = columns (square map)\n","n_rows = 20\n","n_cols = 20\n","\n","# set random vector for each nodes\n","# we will have 3 x 20 x 20 matrix\n","net = np.random.random((n_dim, n_cols, n_rows))\n","net_ori = net.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FuAeNkiJxCM0"},"source":["# Prepare the function\n","n_iter = 5000                                   # no. of maximum iteration\n","lr_init = 0.1                                   # initial learning rate / step size\n","sigma_init = max(n_rows, n_cols)/2              # initial sigma (region of influence)\n","time_constant = n_iter/np.log(sigma_init)       # initial time constant (for decaying formula)\n","\n","# Distance function\n","# You can write the proper distance based on your need\n","def distance(x, y, types):\n","    if types==\"eucledian\":\n","        dist = np.linalg.norm(x-y)\n","    elif types==\"manhattan\":\n","        dist = np.sum(np.abs(x-y))\n","\n","    return dist\n","\n","# The BMU function\n","# To find the closest distance between data and node's weight\n","def findBMU(vec, net):\n","    net_ = net.copy()\n","    n_dim, n_cols, n_rows = net_.shape\n","    list_distance = []\n","    list_coordinates = []\n","    net_mat = np.column_stack([net_[i].flatten() for i in range(n_dim)])\n","\n","    for y in range(n_rows):\n","        for x in range(n_cols):\n","            # Define unit/node\n","            index_unit = x + n_cols*y\n","            unit = net_mat[index_unit]\n","            \n","            # Obtain distance\n","            dist = distance(vec, unit, types=\"eucledian\")\n","\n","            # save to list\n","            list_distance.append(dist)\n","            list_coordinates.append(np.array([x, y]))\n","\n","    # find BMU\n","    BMU_index = np.argmin(list_distance)\n","    BMU = list_coordinates[BMU_index]\n","\n","    return (BMU, BMU_index)\n","\n","# Decaying value over time\n","def decaySigma(time, sigma_init=sigma_init, time_constant=time_constant):\n","    return sigma_init*np.exp(-time/time_constant)\n","\n","def decayLR(time, LR_init=lr_init, time_constant=time_constant):\n","    return lr_init*np.exp(-time/time_constant)\n","\n","def decayIR(dist, sigma):\n","    return np.exp(-(dist**2)/(2*(sigma**2)))\n","\n","# Update the weight vector\n","def updateNet(vec, BMU, net, lr, sigma):\n","    net_ = net.copy()\n","    n_dim, n_cols, n_rows = net_.shape\n","    net_mat = np.column_stack([net_[i].flatten() for i in range(n_dim)])\n","\n","    for y in range(n_rows):\n","        for x in range(n_cols):\n","            index_unit = x + n_cols*y\n","\n","            # obtain distance of influence\n","            dist = distance(np.array([x, y]), BMU, types=\"eucledian\")\n","\n","            if dist <= sigma:\n","                net_mat[index_unit] += decayIR(dist, sigma) * lr * (vec - net_mat[index_unit])\n","\n","    # reshape\n","    new_net = net_mat.T.reshape((n_dim, n_cols, n_rows))\n","    \n","    return new_net\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jy8ssOcS9lb2"},"source":["# Start the iteration\n","for t in range(n_iter):\n","    # 1. Select random input vector\n","    index_to_check = np.random.randint(n_data)\n","    vector_to_check = data_color_norm[index_to_check, :]\n","\n","    # 2. Calculate BMU\n","    BMU, BMU_index = findBMU(vector_to_check, net)\n","    \n","    # 3. Adjust Networks\n","    sigma_current = decaySigma(t)\n","    lr_current = decayLR(t)\n","    new_net = updateNet(vector_to_check, BMU, net, lr_current, sigma_current)\n","    net = new_net\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEVlC9IbGHlu"},"source":["fig, ax = plt.subplots(1, 2)\n","fig.set_figheight(6)\n","fig.set_figwidth(14)\n","\n","ax[0].set_title('Initial')\n","ax[0].set_xlim((0, n_cols+1))\n","ax[0].set_ylim((0, n_rows+1))\n","for x in range(1, n_cols+1):\n","    for y in range(1, n_rows+1):\n","        ax[0].add_patch(patches.Rectangle((x-0.5, y-0.5), 1, 1,\n","                                       facecolor = net_ori[:, x-1, y-1],\n","                                       alpha = 1., edgecolor = \"black\"))\n","        \n","\n","ax[1].set_title('Final')\n","ax[1].set_xlim((0, n_cols+1))\n","ax[1].set_ylim((0, n_rows+1))\n","for x in range(1, n_cols+1):\n","    for y in range(1, n_rows+1):\n","        ax[1].add_patch(patches.Rectangle((x-0.5, y-0.5), 1, 1,\n","                                       facecolor = net[:, x-1, y-1],\n","                                       alpha = 1., edgecolor = \"black\"))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZhFra57j3TCZ"},"source":["See how nicely we just organized the color."]},{"cell_type":"markdown","metadata":{"id":"NIQURv8PHRob"},"source":["### **SOM with MiniSOM**\n","\n","- There exist various implementations of SOM. One interesting implementation is the `MiniSom` (see [MiniSOM](https://github.com/JustGlowing/minisom)). \n","- We will then use `MiniSom` on our color data set. The first step is to install `MiniSom`:"]},{"cell_type":"code","metadata":{"id":"9zB-_i-sxdKV"},"source":["# Install minisom library\n","!pip install minisom\n","from minisom import MiniSom"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQ9sq0bb3rc7"},"source":["The following cell generates our data randomly. The size of the SOM is also defined here."]},{"cell_type":"code","metadata":{"id":"N7AlQBt45K6o"},"source":["# Create data\n","n_dim = 3\n","n_data = 1000\n","\n","# Create new data (uncomment line 6-7 to create new data)\n","#data_color = np.random.randint(0, 256, (n_data, n_dim))\n","#data_color_norm = data_color/255\n","\n","# Define SOM size\n","size = 20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2kkh71Y3zGz"},"source":["Let's call ```MiniSom``` and then train the network:"]},{"cell_type":"code","metadata":{"id":"0BY4hpUnxniu"},"source":["# Call MiniSOM function\n","som = MiniSom(x=size, y=size, input_len=n_dim,\n","              sigma=10.0, learning_rate=0.1, random_seed=10)\n","\n","# Set random weights\n","som.random_weights_init(data_color_norm)\n","\n","# Train the SOM\n","som.train_batch(data=data_color_norm, num_iteration=5000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_smN6Nw35EA"},"source":["The following cell plots the result:"]},{"cell_type":"code","metadata":{"id":"R_ZK_gPrDMon"},"source":["# Get nodes weights\n","W = som.get_weights()\n","\n","# Plot\n","fig, ax = plt.subplots(1, 1)\n","fig.set_figheight(6)\n","fig.set_figwidth(7)\n","     \n","ax.set_title('Final')\n","ax.imshow(W, origin='lower')\n","ax.grid(False)\n","ax.set_xticks([i for i in range(size)])\n","ax.set_yticks([i for i in range(size)])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dH7Ezr196inY"},"source":["It would be nice if we can somehow partition the result into several clusters to allow easier interpretation. The following two cells do just that:"]},{"cell_type":"code","metadata":{"id":"e72ITqD27rq3"},"source":["# Reshape weights\n","W_tf_1 = W.reshape((size*size, n_dim))\n","\n","# Perform clustering\n","n_clust = 5\n","kmeans_color = KMeans(n_clusters=n_clust, random_state=0).fit(W_tf_1)\n","labels_ = kmeans_color.labels_.reshape((size, size))   # reshape labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARPQ1wBR6lI7"},"source":["# Line 1-10 -> plotting color\n","fig, ax = plt.subplots(1, 1)\n","fig.set_figheight(6)\n","fig.set_figwidth(7)\n","     \n","ax.set_title('Final')\n","ax.imshow(W, origin='lower')\n","ax.grid(False)\n","ax.set_xticks([i for i in range(size)])\n","ax.set_yticks([i for i in range(size)])\n","\n","# Line 13-18 -> plotting border line\n","for y in range(0, size-1):\n","    for x in range(0, size-1):\n","        if labels_[x, y] != labels_[x, y+1]:\n","            plt.plot([x, x+1], [y+1, y+1], c='black')\n","        if labels_[x, y] != labels_[x+1, y]:\n","            plt.plot([x+1, x+1], [y, y+1], c='black')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wEolNAifPF77"},"source":["We will do one more step, that is to map the red, green, and blue features:"]},{"cell_type":"code","metadata":{"id":"ieTzGFqP3htu"},"source":["color_names = ['red', 'green', 'blue']\n","\n","plt.figure(figsize=(10, 10))\n","for i, f in enumerate(color_names):\n","    plt.subplot(3, 3, i+1)\n","    plt.title(f)\n","    plt.pcolor(W[:,:,i], cmap='coolwarm')\n","    plt.xticks(np.arange(size+1))\n","    plt.yticks(np.arange(size+1))\n","    for y in range(0, size-1):\n","        for x in range(0, size-1):\n","            if labels_[x, y] != labels_[x, y+1]:\n","                plt.plot([x+0.5, x+1.5], [y+1.5, y+1.5], c='black')\n","            if labels_[x, y] != labels_[x+1, y]:\n","                plt.plot([x+1.5, x+1.5], [y+0.5, y+1.5], c='black')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOnyBQfKJlQO"},"source":["---\n","### **Airfoil Noise**\n","We will use SOM to discover important trend from engineering data sets. Our data set is an airfoil noise experiment ([source](https://archive.ics.uci.edu/ml/datasets/airfoil+self-noise)), with the following features and output:\n","\n","**Input features:**\n","- f: Frequency in Hertzs [Hz].\n","- alpha: Angle of attack (AoA, $\\alpha$), in degrees [¬∞].\n","- c: Chord length, in meters [m].\n","- U_infinity: Free-stream velocity, in meters per second [m/s].\n","- delta: Suction side displacement thickness (ùõø), in meters [m].\n","\n","\n","**Output:**\n","- SSPL: Scaled sound pressure level, in decibels [dB].\n"]},{"cell_type":"markdown","metadata":{"id":"WiXoRqYfAFvG"},"source":["Let's begin by downloading the data to our notebook:"]},{"cell_type":"code","metadata":{"id":"78EnPqmjJm5o"},"source":["# Load the data\n","df = pd.read_csv('AirfoilSelfNoise.csv')\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xqf-XOLEmS-A"},"source":["The next step is to scale our data and also the size of our map:"]},{"cell_type":"code","metadata":{"id":"p6mWQbL0KBMJ"},"source":["# Use SkLearn to scale it\n","# We use standard scaler (mean=0.0, std=1.0)\n","from sklearn.preprocessing import scale\n","\n","# Create data\n","n_dim = 6               # the dimension is 6 (5 feature + 1 objective)\n","n_data = len(df)        # number of data\n","\n","scale_df = scale(df)    # scale the data\n","\n","# Define SOM size\n","size = 20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mL5dOHdXmaTq"},"source":["The following cell defines our SOM and also train it with our data set:"]},{"cell_type":"code","metadata":{"id":"uZt4a29jKPPv"},"source":["# Perform the SOM\n","som = MiniSom(x=size, y=size, input_len=n_dim,\n","              sigma=10.0, learning_rate=0.1, random_seed=10)\n","\n","# Random weights\n","som.random_weights_init(scale_df)\n","\n","# Train the SOM\n","som.train_batch(data=scale_df, num_iteration=5000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugeP4ePWm4Z1"},"source":["To allow easier interpretation, we can cluster our data into several clusters:"]},{"cell_type":"code","metadata":{"id":"v05di_PPKVGi"},"source":["# Now, we perform the clustering of SOM Maps\n","\n","# Reshape weights\n","W = som.get_weights()\n","W_tf_1 = W.reshape((size*size, n_dim))\n","\n","# Perform clustering\n","n_clust = 5\n","kmeans_df = KMeans(n_clusters=n_clust, random_state=0).fit(W_tf_1)\n","labels_ = kmeans_df.labels_.reshape((size, size))   # reshape labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zW0OuXi4mqtH"},"source":["Finally, let's plot our data:"]},{"cell_type":"code","metadata":{"id":"titFApZ8Kkf_"},"source":["features_names = ['f', 'alpha', 'c','U_infinity','delta','SSPL']\n","\n","plt.figure(figsize=(10, 10))\n","for i, f in enumerate(features_names):\n","    plt.subplot(3, 3, i+1)\n","    plt.title(f)\n","    plt.pcolor(W[:,:,i], cmap='coolwarm')\n","    plt.xticks(np.arange(size+1))\n","    plt.yticks(np.arange(size+1))\n","    for y in range(0, size-1):\n","        for x in range(0, size-1):\n","            if labels_[x, y] != labels_[x, y+1]:\n","                plt.plot([x+0.5, x+1.5], [y+1.5, y+1.5], c='black')\n","            if labels_[x, y] != labels_[x+1, y]:\n","                plt.plot([x+1.5, x+1.5], [y+0.5, y+1.5], c='black')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yx07r3rAQNmv"},"source":["How to interpret this map? \n","- First, remember that the data points are clustered based on their topological features. \n","- Next, we can see the relationship between the features and the output by observing the values taken at each node/cluster. \n","\n","For example:\n","\n","*   alpha and delta are highly correlated.\n","*   High velocity typically yields high SSPL.\n"]},{"cell_type":"markdown","metadata":{"id":"hHbNq0XsRGX3"},"source":["---\n","### **Compressor Data**\n","Our second data set is a compressor data in which our aim to uncover the relationship between the blade configuration and the performance metrics (i.e., total pressure ratio and efficiency). What we are analyzing is an axial compressor similar to the following figure:\n","\n","<center>\n","<img src=\"https://cdn.sanity.io/images/0vv8moc6/turbomag/2f6066031c27080b8467e74ab5a16b6431dc42ec-650x365.jpg/sanity_1221-compressor_blades.jpg?w=1500&fit=max&auto=format\"/>\n","<br>\n","<img src=\"https://hygprogramming.files.wordpress.com/2014/12/deformation_modes.jpg?w=508&h=320\"/>\n","</center>\n","\n","\n","[Figure source-1](https://cdn.sanity.io/images/0vv8moc6/turbomag/2f6066031c27080b8467e74ab5a16b6431dc42ec-650x365.jpg/sanity_1221-compressor_blades.jpg?w=1500&fit=max&auto=format), \n","[Figure source-2](https://hygprogramming.files.wordpress.com/2014/12/deformation_modes.jpg?w=508&h=320)\n","\n","The features for this problem are listed below:\n","\n","**Input features:**\n","*   Sweep [¬∞]\n","*   Lean [¬∞]\n","*   Skew [¬∞]\n","\n","**Output:**\n","*   Total pressure ratio [-]\n","*   Efficiency [-]\n","\n","Let's load our data set:\n"]},{"cell_type":"code","metadata":{"id":"NoAJT6x6RF3b"},"source":["# Load the data \n","compressor_df = pd.read_csv('compressor_data.csv')\n","compressor_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rvja6WOWpIXo"},"source":["Some rearrangements of our data (the TPR and efficiency are originally negative values because this data was used in the context of minimization of these two outputs):"]},{"cell_type":"code","metadata":{"id":"eKDMGmqBReEB"},"source":["# Wrangling Data\n","compressor_df['TPR'] = -compressor_df['TPR']\n","compressor_df['efficiency'] = -compressor_df['efficiency']\n","compressor_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2tMUAuDdpYUw"},"source":["As usual, we should scale our data first and define the size of the map:"]},{"cell_type":"code","metadata":{"id":"UTEYVxLMRuKi"},"source":["from sklearn.preprocessing import scale\n","\n","# Create data\n","n_dim = 5\n","n_data = len(compressor_df)\n","\n","scaled_compressor_df = scale(compressor_df)\n","\n","# Define SOM size\n","size = 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e4wmcqqjpgBM"},"source":["Please execute all of the following cells until you obtain the map:"]},{"cell_type":"code","metadata":{"id":"8MQSqnwIR9Jb"},"source":["# Perform the SOM\n","som = MiniSom(x=size, y=size, input_len=n_dim,\n","              sigma=5.0, learning_rate=0.1, random_seed=10)\n","\n","som.random_weights_init(scaled_compressor_df)\n","\n","som.train_batch(data=scaled_compressor_df, num_iteration=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSsRTZVbSOHn"},"source":["# Reshape the weights and perform the clustering\n","\n","# Reshape weights\n","W = som.get_weights()\n","W_tf_1 = W.reshape((size*size, n_dim))\n","\n","# Perform clustering\n","from sklearn.cluster import KMeans\n","\n","n_clust = 4\n","kmeans_df = KMeans(n_clusters=n_clust, random_state=0).fit(W_tf_1)\n","labels_ = kmeans_df.labels_.reshape((size, size))   # reshape labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"EHgKr5KKScJU","executionInfo":{"status":"ok","timestamp":1627809793490,"user_tz":-420,"elapsed":977,"user":{"displayName":"MDO-ML ITB","photoUrl":"","userId":"11799730835295313374"}},"outputId":"4df6f882-2df4-4ba7-ae57-693d0e3469f7"},"source":["features_names = ['sweep', 'lean', 'skew', 'TPR', 'efficiency']\n","\n","plt.figure(figsize=(10, 10))\n","for i, f in enumerate(features_names):\n","    plt.subplot(3, 3, i+1)\n","    plt.title(f)\n","    plt.pcolor(W[:,:,i], cmap='coolwarm')\n","    plt.xticks(np.arange(size+1))\n","    plt.yticks(np.arange(size+1))\n","    for y in range(0, size-1):\n","        for x in range(0, size-1):\n","            if labels_[x, y] != labels_[x, y+1]:\n","                plt.plot([x+0.5, x+1.5], [y+1.5, y+1.5], c='black')\n","            if labels_[x, y] != labels_[x+1, y]:\n","                plt.plot([x+1.5, x+1.5], [y+0.5, y+1.5], c='black')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x720 with 5 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"se6aCNI4VrYB"},"source":["Let's try interpreting this map:\n","\n","*   The objective functions (i.e., TPR and efficiency) are conflicting with each other. That is, you can obtain high TPR but the efficiency will decrease.\n","*   Efficiency can be maximized by increasing the lean and the skew (not so clear for sweep, though).\n","*   Finally, high TPR can be obtained by decreasing the sweep, the lean, and the skew. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Lsrtj2_bqp7q"},"source":["**That's all! Thanks for your attention**"]}]}