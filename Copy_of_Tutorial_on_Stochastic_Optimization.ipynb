{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Tutorial on Stochastic Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adrianxwu/MDO-ML-IVC-ITB-2021/blob/main/Copy_of_Tutorial_on_Stochastic_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zxSlNZXaGXS"
      },
      "source": [
        "# **TUTORIAL 2: STOCHASTIC OPTIMIZATION USING EVOLUTIONARY ALGORITHM**\n",
        "\n",
        "**Multidisciplinary Optimization and Machine Learning for Engineering Design International Virtual Course**\n",
        "\n",
        "**19 July - 5 August 2021**\n",
        "\n",
        "Written by: Yohanes Bimo Dwianto\n",
        "\n",
        "Based on the lecture material by: Hemant Kumar Singh, Ph.D. (University of New South Wales Canberra, Australia)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgrWTwN4KQjh"
      },
      "source": [
        "---\n",
        "---\n",
        "# 1. Theory Overview\n",
        "\n",
        "---\n",
        "## General Form of Optimization\n",
        "Suppose that we want to do a minimization problem, the general form of an optimization problem can be written as follows:\n",
        "\n",
        "\\begin{equation} \n",
        "\\begin{array}{ll}\n",
        "\\mbox{Minimize}     & f_i(\\boldsymbol{x}),\\ i=1,\\ldots,N_{obj} \\\\\n",
        "\\mbox{subject to}   & g_j(\\boldsymbol{x}) \\le 0,\\ j=1,\\ldots,N_{ineq}\\\\\n",
        "& h_k(\\boldsymbol{x}) = 0,\\ k=1,\\ldots,N_{eq}\\\\\n",
        "& \\boldsymbol{x}_L \\le \\boldsymbol{x} \\le \\boldsymbol{x}_U\n",
        "\\end{array}    \n",
        "\\end{equation}\n",
        "\n",
        "where \n",
        "* $\\boldsymbol{x}$ denotes the $N_{var}$-dimensional solution vector\n",
        "* $f_i$, $g_j$, and $h_k$ denote the $i^{th}$ objective function, the $j^{th}$ inequality constraint, and the $k^{th}$ equality constraint, respectively\n",
        "* $N_{obj}$ denotes the number of objective functions\n",
        "* $N_{ineq}$ and $N_{eq}$ denote the number of inequality constraints and equality constraints, respectively. Then, the number of all constraints is $N_{con}=N_{ineq}+N_{eq}$\n",
        "* Both $\\boldsymbol{x}_L$ and $\\boldsymbol{x}_U$ denote the lower and upper bounds of the search space, respectively. \n",
        "* In this tutorial, we will focus only on **single-objective optimization ($N_{obj}=1$)**.\n",
        "---\n",
        "## Stochastic Optimization\n",
        "* Stochastic optimization can be defined as minimizing or maximizing a problem with the help of randomness.\n",
        "* Statistical analysis is needed to ensure its performance.\n",
        "* Some popular methods: evolutionary algorithm (EA), particle swarm optimization, ant colony optimization, etc.\n",
        "---\n",
        "## Evolutionary Algorithm: General Framework\n",
        "* Population initialization\n",
        "* Calculation of fitness values, $F$\n",
        "* Parental selection\n",
        "* Crossover/ recombination\n",
        "* Mutation\n",
        "* Environmental selection\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1T8HdBIVAHh_1-GRW28ABp0TiZN8EZeuR)\n",
        "\n",
        "In the case of **unconstrained** optimization, the **fitness value** of each individual, $F(\\boldsymbol{x})$, **equals** its **objective value**.\n",
        "\n",
        "In the case of **constrained** optimization, the **fitness value** of each individual, $F(\\boldsymbol{x})$, is a **combination of its objective and constraint violation values**; one example is described in the next section.\n",
        "\n",
        "---\n",
        "## Constrained Handling Technique (CHT)\n",
        "* EA was initially designed to solve unconstrained problems. However, various constrained EAs have been developed to handle problems with constraints. To this end, a constraint handling technique needs to be implemented in order to solve constrained problems. In this tutorial, we will give you a simple constraint handling technique so that you can appreciate the main idea of constrained EA. \n",
        "* In EA, equality constraints are often modified into inequality constraints, reads as follows\n",
        "\\begin{equation}\n",
        "g_{N_{ineq}+k}(\\boldsymbol{x}) = \\lvert h_k(\\boldsymbol{x}) \\rvert-\\epsilon \\le 0, \\ k=1,\\ldots,N_{eq}\n",
        "\\end{equation}\n",
        "where $\\epsilon$ is a user-defined small number, typically $\\epsilon=1 \\times 10^{-4}$.\n",
        "\n",
        "* Let's talk about **Penalty function**, which is one of the simplest and probably the most popular CHT technique. A penalty function is generally expressed as follows\n",
        "\\begin{equation} \n",
        "    F(\\boldsymbol{x})= f_r(\\boldsymbol{x}) + p(\\boldsymbol{x})\n",
        "\\end{equation}\n",
        "where $f_r(\\boldsymbol{x})$ is the search in objective space and $p(\\boldsymbol{x})$ is the penalty function, representing the search in constraint space, usually based on constraint violation value, $\\nu_i(\\boldsymbol{x})$,\n",
        "\\begin{equation} \n",
        "\t\\nu_i(\\boldsymbol{x}) = \\max(0,g_i(\\boldsymbol{x}))\n",
        "\\end{equation}\n",
        "---\n",
        "## Statistical Analysis: Box Plot\n",
        "* Due to its random nature, multiple runs need to be conducted in EA to ensure its performance. This is particularly useful in the context of research when you want to compare various EA algorithms. Typically, you want to compare the value of the best solution observed during one run of EA,\n",
        "* We can use boxplot for visualizing the performance of EA on multiple independent runs. A box plot visualizes the optimum objective values obtained by all EA runs in statistical form, as shown in the figure below.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1ZaPBo4W9UB2xK_fGN9A4RgnDsd1CypFX)\n",
        "\n",
        "* \"maximum\", median, and \"minimum\" values are the most common values to be considered in the performance analysis.\n",
        "\n",
        "* When comparing algorithms, it is highly recommended to use **statistical hypothesis test** (e.g. Mann-Whitney test) to check whether an algorithm is significantly better than other algorithms or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohccJq2Ibnyi"
      },
      "source": [
        "---\n",
        "---\n",
        "# 2. Python Coding of Real-coded Genetic Algorithm (Unconstrained and Constrained Optimization)\n",
        "In this tutorial, we will use Real-coded Genetic Algorithm (RGA), which employs real values to encode the design variables. Here's what you will need to define first:\n",
        "## Problem Inputs:\n",
        "* Objective function\n",
        "* Constraint function(s), if any\n",
        "* Number of design variables, $N_{var}$\n",
        "* Number of constraints, $N_{con}$\n",
        "* Search space: lower bound $lb$ and upper bound $ub$\n",
        "\n",
        "## RGA Inputs\n",
        "* Population size, $N_{pop}$\n",
        "* Number of generations, $N_{gen}$\n",
        "* Crossover probability, $P_{cross}$\n",
        "* Mutation probability, $P_{mut}$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ey5RVLINx2E"
      },
      "source": [
        "---\n",
        "## Importing necessary packages\n",
        "\n",
        "We will code a simple GA from stratch! So you only need to import ```numpy``` and ```matplotlib``` (for visualization):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_SL5l8meeT8"
      },
      "source": [
        "#@title Importing Packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_KoX8o9Mrao"
      },
      "source": [
        "---\n",
        "## Defining Test Problem: \n",
        "\n",
        "**Unconstrained Problem: Ackley Problem**\n",
        "\n",
        "Let's consider a very difficult problem, namely the **Ackley** problem. Because of its multimodality (i.e. many local optima), the Ackley problem serves as a good test problem for demonstrating the strenght of EA in solving multimodal problems. See the visualization below:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1CgmQ7cMcswJBi7By4nepoB1JwsGAkM0g)\n",
        "\n",
        "It has the following characteristics:\n",
        "* Ackley has number of design variables, $N_{var}$, which may vary from one to infinity. \n",
        "* This problem has a minimum objective value of zero, located in $\\boldsymbol{x}=\\{0,\\ldots,0\\}$.\n",
        "* It is expressed as follows:\n",
        "$$f(\\boldsymbol{x})=-a \\exp{ \\left(-b\\sqrt{\\frac{1}{N_{var}}\\sum^{N_{var}}_{i=1}x^{2}_{i}} \\right)}-\\exp{\\left( \\frac{1}{N_{var}} \\sum^{N_{var}}_{i=1} \\cos{\\left( cx_i \\right)}\\right)}+a+\\exp{\\left( 1 \\right)}$$ \n",
        "where $a=20$, $b=0.2$, and $c=2\\pi$. \n",
        "* The function is evaluated at the hypercube $x_{i} \\in [-32.768, 32.768]$, for all $i=1,\\ldots,N_{var}$\n",
        "\n",
        "**Reference**:\n",
        "https://www.sfu.ca/~ssurjano/ackley.html\n",
        "<br><br>\n",
        "**Constrained problem: Welded Beam** \n",
        "Our second test problem is the constrained **welded beam** problem, which creates extra challenges for EAs because we need CHT to find the feasible optimal solution. See the illustration of the welded beam problem below:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=157ZHCeL7ndL1virle5u72Tl9tyMEBp98)\n",
        "\n",
        "The welded beam problem has the following characteristics:\n",
        "* Welded Beam problem has four design variables, $N_{var}=4$ and six constraints, $N_{con}=6$ (all inequality constraints). \n",
        "* The currently found minimum feasible objective value is $f(\\boldsymbol{x}_{opt})=1.724855673$, located in $\\boldsymbol{x}_{opt}=\\{0.20573,3.470489,9.036624,0.20573\\}^T$\n",
        "* The objective function is to minimize:\n",
        "$$f(\\boldsymbol{x})=1.10471x_1^2x_2+0.04811x_3x_4(14.0+x_2)$$\n",
        "subject to:\n",
        "$$g_1(\\boldsymbol{x})=\\tau(\\boldsymbol{x})-\\tau_{max} \\le 0$$\n",
        "$$g_2(\\boldsymbol{x})=\\sigma(\\boldsymbol{x})-\\sigma_{max} \\le 0$$\n",
        "$$g_3(\\boldsymbol{x})=x_1-x_4 \\le 0$$\n",
        "$$g_4(\\boldsymbol{x})=0.10471x_1^2+0.04811x_3x_4(14.0+x_2)-5 \\le 0$$\n",
        "$$g_5(\\boldsymbol{x})=\\delta(\\boldsymbol{x})-\\delta_{max} \\le 0$$\n",
        "$$g_6(\\boldsymbol{x})=P-P_c(\\boldsymbol{x} \\le 0$$\n",
        "where\n",
        "$$\\tau(\\boldsymbol{x})=\\sqrt{(\\tau')^2+2\\tau'\\tau''\\frac{x_2}{2R}+(\\tau'')^2}$$\n",
        "$$\\tau'=\\frac{P}{\\sqrt{2}x_1x_2},\\tau''=\\frac{MR}{J},M=P\\left( L+\\frac{x_2}{2}\\right)$$\n",
        "$$R=\\sqrt{\\frac{x_2^2}{4}+\\left( \\frac{x_1+x_3}{2}\\right)^2}$$\n",
        "$$J=2\\left(\\sqrt{2}x_1x_2\\left[ \\frac{x_2^2}{12}+\\left( \\frac{x_1+x_3}{2}\\right)^2\\right]\\right)$$\n",
        "$$\\sigma(\\boldsymbol{x})=\\frac{6PL}{x_4x_3^2},\\delta(\\boldsymbol{x})=\\frac{4PL}{x_3x_4^2}$$\n",
        "$$P_c(\\boldsymbol{x})=\\frac{4.013E\\sqrt{\\frac{x_3^2x_4^6}{36}}}{L^2}\\left( 1-\\frac{x_3}{2L}\\sqrt{\\frac{E}{4G}} \\right)$$\n",
        "$$P=6000 \\ lb, L=14 \\ in, E=30 \\times 10^6 \\ psi, G=12 \\times 10^6 \\ psi$$\n",
        "$$\\tau_{max}=13600 \\ psi, \\sigma_{max}=30000 \\ psi, \\delta_{max}=0.25 \\ in$$\n",
        "<br><br>\n",
        "**Reference**:\n",
        "Rao, Singiresu S. Engineering optimization: theory and practice. John Wiley & Sons, 2019.\n",
        "\n",
        "Let's define the two problems by executing the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcnHpUoPMbz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a747be-f598-4d79-941b-362713afd5dd"
      },
      "source": [
        "#@title Test Problems\n",
        "def testproblem(indi_denorm,ncon,fun_name):\n",
        "    nvar = indi_denorm.shape[0] # number of design variables\n",
        "\n",
        "    if fun_name == 'ackley':\n",
        "        a = 20\n",
        "        b = 0.2\n",
        "        c = 2*np.pi\n",
        "        \n",
        "        f = -a*np.exp(-b*np.sqrt(1/nvar*np.sum(indi_denorm**2))) - \\\n",
        "            np.exp(1/nvar*np.sum(np.cos(c*indi_denorm))) + a + np.exp(1)\n",
        "\n",
        "    elif fun_name == 'weldedbeam':\n",
        "\n",
        "        E = 30e6\n",
        "        G = 12e6\n",
        "        L = 14\n",
        "        taw_max = 13600\n",
        "        sigma_max = 30000\n",
        "        delta_max = 0.25\n",
        "        P = 6000\n",
        "        c1 = 0.10471\n",
        "        c2 = 0.04811\n",
        "        c3 = 1\n",
        "\n",
        "        Vweld = indi_denorm[0]**2*indi_denorm[1]\n",
        "        Vbar = indi_denorm[2]*indi_denorm[3]*(L+indi_denorm[1])\n",
        "\n",
        "        taw_d = P/(np.sqrt(2)*indi_denorm[0]*indi_denorm[1])\n",
        "        M = P*(L+indi_denorm[1]/2)\n",
        "        R = np.sqrt(indi_denorm[1]**2/4+((indi_denorm[0]+indi_denorm[2])/2)**2)\n",
        "        J = 2*(indi_denorm[0]*indi_denorm[1]*np.sqrt(2)*(indi_denorm[1]**2/12 + ((indi_denorm[0]+indi_denorm[2])/2)**2))\n",
        "        taw_dd = M*R/J\n",
        "        taw = np.sqrt(taw_d**2 + 2*taw_d*taw_dd*indi_denorm[1]/(2*R) + taw_dd**2)\n",
        "        sigma = 6*P*L/(indi_denorm[3]*indi_denorm[2]**2)\n",
        "        delta = 4*P*L**3/(E*indi_denorm[3]*indi_denorm[2]**3)\n",
        "        Pc = 4.013*E*indi_denorm[2]*indi_denorm[3]**3/(6*L**2)*(1-indi_denorm[2]/(2*L)*np.sqrt(E/(4*G)))\n",
        "\n",
        "        f = (c1+c3)*Vweld + c2*Vbar\n",
        "        g = np.zeros(ncon)\n",
        "        g[0] = taw - taw_max\n",
        "        g[1] = sigma - sigma_max\n",
        "        g[2] = indi_denorm[0] - indi_denorm[3]\n",
        "        g[3] = P - Pc\n",
        "        g[4] = delta - delta_max\n",
        "        g[5] = c1*indi_denorm[0]**2 + c2*indi_denorm[2]*indi_denorm[3]*(L+indi_denorm[1]) - 5\n",
        "\n",
        "    if ncon == 0:\n",
        "        return f\n",
        "    else:\n",
        "        return f,g\n",
        "\n",
        "# TESTING - ACKLEY PROBLEM   \n",
        "nvar = 2\n",
        "ncon = 0\n",
        "x = np.array([0,0])\n",
        "fun_name = 'ackley'\n",
        "f = testproblem(x,ncon,fun_name)\n",
        "print('ACKLEY PROBLEM')\n",
        "print('Optimum objective value:',f)\n",
        "\n",
        "# TESTING - WELDED BEAM PROBLEM   \n",
        "#lb = np.array([0.125,0.1,0.1,0.1])\n",
        "#ub = np.array([2,10,10,2])\n",
        "nvar = 4\n",
        "ncon = 6\n",
        "x = np.array([0.20573,3.470489,9.036624,0.20573])\n",
        "fun_name = 'weldedbeam'\n",
        "f = testproblem(x,ncon,fun_name)\n",
        "print('\\nWELDED BEAM PROBLEM')\n",
        "print('Currently obtained optimum objective:',f[0])\n",
        "print('Currently obtained optimum constraint:',f[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACKLEY PROBLEM\n",
            "Optimum objective value: 4.440892098500626e-16\n",
            "\n",
            "WELDED BEAM PROBLEM\n",
            "Currently obtained optimum objective: 1.7248556738155942\n",
            "Currently obtained optimum constraint: [-0.02539959 -0.05312238  0.         -0.03155555 -0.23554035 -3.43298099]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvjuW_Tfg02E"
      },
      "source": [
        "---\n",
        "## Population Initialization\n",
        "The initial population can be generated either randomly or by using design of experiment (DoE) techniques such as Latin Hypercube sampling (this is the most common), Sobol sequences, Halton sequences, etc. The parameters that we define are as follows:\n",
        "* population size, $N_{pop}$ and\n",
        "* number of design variables, $N_{var}$.\n",
        "\n",
        "It is common to generate population in normalized values between zero and one.\n",
        "\n",
        "The cell below demonstrates a simple code to randomly generate initial population (if you set ```nvar=2``` then you can visualize the initial population):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0ZIKgUEhSb5"
      },
      "source": [
        "#@title Population Initialization\n",
        "# Population Initialization\n",
        "npop = 10\n",
        "nvar = 2\n",
        "population = np.random.rand(npop,nvar)\n",
        "print(population)\n",
        "\n",
        "if nvar == 2:\n",
        "    fig = plt.figure()\n",
        "    plt.scatter(population[:,0],population[:,1])\n",
        "    plt.xlabel(r'$x_1$')\n",
        "    plt.ylabel(r'$x_2$')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAo8_doNTjux"
      },
      "source": [
        "---\n",
        "## Denormalization\n",
        "Since the population is defined in a normalized value from zero to one, each variable of the individual in the population needs to be denormalized first into the real values of design variables with the following expression\n",
        "$$x_{j,denorm} = x_{j,L} + (x_{j,U}-x_{j,L}) x_{j,norm}$$\n",
        "\n",
        "We use the below function to perform the denormalization operation by using the lower and the upper bound of the Ackley function as an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEVR39eQM_ar"
      },
      "source": [
        "#@title Denormalization\n",
        "def calc_denorm(individual,lb,ub):\n",
        "\tindi_denorm = lb + (ub-lb)*individual\n",
        "\treturn indi_denorm\n",
        "\n",
        "lb = -32.768*np.ones(nvar) # lower bound\n",
        "ub = 32.768*np.ones(nvar)  # upper bound\n",
        "x = population[0,:]\n",
        "print('Normalized x:',x)\n",
        "x1 = calc_denorm(x,lb,ub)\n",
        "print('Denormalized x:',x1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGCXMsj0ivrC"
      },
      "source": [
        "---\n",
        "## Calculating Objective and Constraint function values\n",
        "The objective and constraint function values are then evaluated based on the defined optimization problem. In the Ackley problem, only the objective function value is evaluated since no constraint is involved. Let's do a simple test, that is, we will evaluate the objective and constraint values of a random initial population for the Ackley and the welded beam problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tLpk5NkNGas"
      },
      "source": [
        "#@title Calculating Objective and Constraint\n",
        "def calc_obj_con(population,fun_name,ncon,lb,ub,cht_type=None):\n",
        "    npop,nvar = population.shape\n",
        "    objval = np.zeros(npop)         # initializing storing array for objective values\n",
        "    if ncon == 0:\n",
        "        conval = np.zeros((npop,1)) # dummy constraint for unconstrained problem\n",
        "    else:\n",
        "        conval = np.zeros((npop,ncon)) # initializing storing array for constraint values\n",
        "      \n",
        "    for i in range(npop):\n",
        "        individual = population[i,:]\n",
        "        indi_denorm = calc_denorm(individual,lb,ub)\n",
        "        if ncon == 0:\n",
        "           objval[i] = testproblem(indi_denorm,ncon,fun_name)\n",
        "        else:\n",
        "           objval[i],conval[i,:] = testproblem(indi_denorm,ncon,fun_name)\n",
        "\n",
        "    return objval,conval\n",
        "\n",
        "# Testing ACKLEY Problem\n",
        "fun_name = 'ackley'\n",
        "nvar = 2\n",
        "ncon = 0\n",
        "lb = -32.768*np.ones(nvar) # lower bound\n",
        "ub = 32.768*np.ones(nvar)  # upper bound\n",
        "npop = 10\n",
        "population = np.random.rand(npop,nvar)\n",
        "objval,conval = calc_obj_con(population,fun_name,ncon,lb,ub,cht_type=None)\n",
        "\n",
        "print('ACKLEY PROBLEM')\n",
        "print('objective values:',objval)\n",
        "print('constraint values:',conval)\n",
        "\n",
        "# Testing ACKLEY Problem\n",
        "fun_name = 'weldedbeam'\n",
        "nvar = 4\n",
        "ncon = 6\n",
        "lb = np.array([0.125,0.1,0.1,0.1])\n",
        "ub = np.array([2,10,10,2])\n",
        "npop = 10\n",
        "population = np.random.rand(npop,nvar)\n",
        "objval,conval = calc_obj_con(population,fun_name,ncon,lb,ub,cht_type='SoF')\n",
        "\n",
        "print('\\nWELDED BEAM PROBLEM')\n",
        "print('objective values:',objval)\n",
        "print('constraint values:',conval)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P6USn5dlF4h"
      },
      "source": [
        "---\n",
        "## Calculating Fitness value from Objective and Constraint Values\n",
        "So you just calculated the objectives and constraint value for a random population, what you need to do next is to evaluate the **fitness value** for all the individuals in the population:\n",
        "\n",
        "* The objective and constraint values of each individual in the population are converted into **fitness value** to be assessed in EA. \n",
        "* In the case of **unconstrained problem** such as the Ackley problem, **the fitness value of each individual equals its objective value**.\n",
        "* In the case of constrained problems, the conversion of objective and constraint values to the fitness value primarily depends on the constraint handling technique (CHT). In this tutorial, two CHTs are introduced, namely:\n",
        "\n",
        "1.   the superiority of feasible individual (SoF) [Deb, 2000]: a CHT technique that highly favors feasible individuals\n",
        "2.   the generalized multiple constraint ranking (G-MCR) [Dwianto, 2020]: a CHT technique that balances the search between feasible and infeasible regions\n",
        "\n",
        "**SoF** follows the following expression:\n",
        "$$\n",
        "    F(\\boldsymbol{x})=\\left\\{\n",
        "        \\begin{array}{ll}\n",
        "            f(\\boldsymbol{x}), & \\text{if } CV(\\boldsymbol{x})=0, \\\\\n",
        "            f_{worst} + CV(\\boldsymbol{x}), & \\text{otherwise}.\n",
        "        \\end{array}\n",
        "    \\right.\n",
        "$$\n",
        "where $f_{worst}$ is the worst objective value of feasible individual in the current population. In case there is no feasible individual, $f_{worst}$ is set to zero. $CV(\\boldsymbol{x})$ is the sum of constraint violations, i.e., $CV(\\boldsymbol{x})=\\sum_i^{N_{con}} \\nu_i(\\boldsymbol{x})$\n",
        "\n",
        "while **G-MCR** follows\n",
        "\\begin{equation} \n",
        "    F=\\beta_1 R_f+\\beta_2 (\\eta R_{N_\\nu} + \\gamma \\sum_{i=1}^{m} \\alpha_i R_{\\nu_i})\n",
        "\\end{equation}\n",
        "where $R$ denotes the rank of the corresponding subscript in a population. The subscripts $f$, $N_\\nu$, and $\\nu_i$ indicate the objective value, number of constraints violated, and the $i^{th}$ constraint violation ($\\nu_i=\\max(0,g_i)$). \n",
        "$$\\beta_1=\\sqrt{1-(\\zeta-1)^2},\\ \\beta_2=1-\\beta_1,\\ \\zeta \\equiv \\frac{N_{feas}}{N_{pop}}$$ \n",
        "$$\\alpha_i=\\frac{N_{viol_i}}{N_{pop}},\\ \\gamma=\\frac{1}{\\sum_i^{m} \\alpha_i},\\ \\eta=0$$.\n",
        "<br><br>\n",
        "**REFERENCES**\n",
        "* Deb, Kalyanmoy. \"An efficient constraint handling method for genetic algorithms.\" Computer methods in applied mechanics and engineering 186, no. 2-4 (2000): 311-338.\n",
        "* Dwianto, Yohanes Bimo, Hiroaki Fukumoto, and Akira Oyama. \"Adaptively preserving solutions in both feasible and infeasible regions on generalized multiple constraint ranking.\" In Proceedings of the 2020 Genetic and Evolutionary Computation Conference, pp. 690-698. 2020.\n",
        "\n",
        "The cell below defines our CHT techniques:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUN_2ssQNGS0"
      },
      "source": [
        "#@title Calculating Fitness\n",
        "def evaluate_fitval(objval,conval,cht_type=None):\n",
        "    npop,ncon = conval.shape\n",
        "    fitval = np.zeros(npop)\n",
        "\n",
        "    # Calculating constraint violation fron constraint values\n",
        "    cv = conval.copy()\n",
        "    cv[cv<0] = 0\n",
        "\n",
        "    Nv = np.sum(cv>0,axis=1,dtype='int')  # counting number of constraints violated for each individual\n",
        "    countfeas = np.sum(Nv==0)             # counting the number of feasible individuals\n",
        "\n",
        "    if cht_type is None:\n",
        "        fitval[:] = objval\n",
        "    elif cht_type == 'SoF':\n",
        "        sumcv = np.sum(cv,axis=1) # sum of constraint violations\n",
        "        if countfeas > 0:\n",
        "            objmax = np.amax(objval[np.where(Nv==0)[0]])\n",
        "        else:\n",
        "            objmax = 0\n",
        "        for i in range(npop):\n",
        "            if sumcv[i] == 0.0:\n",
        "                fitval[i] = objval[i]\n",
        "            else:\n",
        "                fitval[i] = objmax + sumcv[i]\n",
        "    elif cht_type == 'G-MCR':\n",
        "        con_check = np.sum(cv>0,axis=0)\n",
        "        Fobj = np.zeros(npop)\n",
        "        Fcon = np.zeros((npop,ncon))\n",
        "        FNv = np.zeros(npop)\n",
        "        for i in range(npop):\n",
        "            Fobj[i] = np.sum(objval<objval[i])\n",
        "            FNv[i] = np.sum(Nv<Nv[i])\n",
        "            for j in range(ncon):\n",
        "                Fcon[i,j] = np.sum(cv[:,j]<cv[i,j])\n",
        "\n",
        "        beta1 = np.sqrt(1-(countfeas/npop-1)**2)\n",
        "        beta2 = 1-beta1\n",
        "        eta = 0\n",
        "        alpha = (1/npop)*con_check\n",
        "        sum_alpha = np.sum(alpha)\t\t\t\n",
        "        if sum_alpha > 0:\n",
        "            gamma = 1/sum_alpha\n",
        "        else:\n",
        "            gamma = npop\n",
        "        fitval[:] = beta1*Fobj + beta2*(eta*FNv + gamma*np.sum(np.multiply(Fcon,alpha),axis=1))\n",
        "\n",
        "    return fitval,cv,Nv\n",
        "\n",
        "fitval,cv,Nv = evaluate_fitval(objval,conval)\n",
        "print('fitness value:',fitval)\n",
        "print('\\nconstraint violation:',cv)\n",
        "print('\\nnumber of constraints violated:',Nv)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89_0ZMzsqhuC"
      },
      "source": [
        "---\n",
        "## Parental Selection\n",
        "Parental selection is a process to select some individuals which are fit for mating and producing offspring. Some popular methods are tournament selection and roulette wheel. In this tutorial, we use the **binary tournament selection** method, which has the following procedure:\n",
        "* select $k=2$ random individuals from the current parent population, and\n",
        "* put the best individual in a mating pool.\n",
        "The procedure stops when there are $N_{pop}$ individuals in the mating pool.\n",
        "\n",
        "Note that this is a random process, that is, you will generate new mating pool each time you rerun the cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBwY9772NGK3"
      },
      "source": [
        "#@title Parental Selection\n",
        "def create_matingpool(population,fitval):\n",
        "\tnpop,nvar = population.shape\n",
        "\tmatingpool = np.zeros((npop,nvar))\n",
        "\tfor kk in range(npop):\n",
        "\t\tip1 = np.random.randint(npop)\n",
        "\t\tip2 = np.random.randint(npop)\n",
        "\t\tif ip2 == ip1:\n",
        "\t\t\twhile ip2 == ip1:\n",
        "\t\t\t\tip2 = np.random.randint(npop)\n",
        "\t\tFt1 = population[ip1-1,:]\n",
        "\t\tFt2 = population[ip2-1,:]\n",
        "\t\tFit1 = fitval[ip1-1]\n",
        "\t\tFit2 = fitval[ip2-1]\n",
        "\t\tif Fit1 < Fit2:\n",
        "\t\t\tmatingpool[kk,:] = Ft1\n",
        "\t\telse:\n",
        "\t\t\tmatingpool[kk,:] = Ft2\n",
        "\t\n",
        "\treturn matingpool\n",
        "\n",
        "matingpool = create_matingpool(population,fitval)\n",
        "print('matingpool:',matingpool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUe7EH8dsKU5"
      },
      "source": [
        "---\n",
        "## Crossover/ Recombination\n",
        "Let's talk about the two main mechanisms that drive EA toward optimal solution, namely, **crossover** and **mutation**; we'll begin with the former.\n",
        "* Crossover/ recombination is performed so that new individuals are expected to yield better fitness values than the old individuals. \n",
        "* In canonical EA, a pair of parents are usually selected from the mating pool to allow them producing a pair of offspring by a crossover method.\n",
        "*This process is repeated until there is a population of offspring with size of $N_{pop}$. In this tutorial, we utilize **simulated binary crossover (SBX)**. Suppose $\\boldsymbol{x}_1 = \\{x_{1,1},\\ldots,x_{d,1}\\}$ and $\\boldsymbol{x}_2 = \\{x_{1,2},\\ldots,x_{d,2}\\}$ are a pair of parents, SBX generates a pair of offspring $\\boldsymbol{c}_1 = \\{c_{1,1},\\ldots,c_{d,1}\\}$ and $\\boldsymbol{c}_2 = \\{c_{1,2},\\ldots,c_{d,2}\\}$ from $\\boldsymbol{x}_1$ and $\\boldsymbol{x}_2$ with the following expression,\n",
        "$$c_{j,1} = 0.5(x_{j,1}+x_{j,2}-\\overline{\\beta} |x_{j,2}-x_{j,1}|)$$\n",
        "$$c_{j,2} = 0.5(x_{j,1}+x_{j,2}+\\overline{\\beta} |x_{j,2}-x_{j,1}|)$$\n",
        "where\n",
        "$$\n",
        "\t\\overline{\\beta} = \\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t(\\alpha u)^{\\frac{1}{\\eta_c+1}}, & \\text{if } u \\leq \\frac{1}{\\alpha}, \\\\\n",
        "\t(\\frac{1}{2-\\alpha u})^{\\frac{1}{\\eta_c+1}}, & \\text{otherwise}.\n",
        "\t\\end{array}\n",
        "\t\\right.\n",
        "$$\n",
        "where $\\alpha = 2-\\beta^{-(\\eta_c+1)}$, and $\\beta$ is calculated as follows,\n",
        "$$\n",
        "\\beta = 1 + \\frac{2}{x_{j,2}-x_{j,1}} \\min(x_{j,1}-x_{j,L},x_{j,U}-x_{j,2}) \\text{.}\n",
        "$$\n",
        "It is assumed that $x_{j,1} < x_{j,2}$ here. The symbols $u$ and $\\eta_c$ denote a random value and distribution index, respectively. Here, we set $\\eta_c=20$ in all experiment.\n",
        "\n",
        "<br><br>\n",
        "**Reference**:\n",
        "Deb, Kalyanmoy, and Ram Bhushan Agrawal. \"Simulated binary crossover for continuous search space.\" Complex systems 9, no. 2 (1995): 115-148.\n",
        "\n",
        "The cell below executes a random crossover process to create a new population of offsprings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-ELuTdXNGBg"
      },
      "source": [
        "#@title Crossover\n",
        "def SBX(indi1,indi2,nvar):\n",
        "    nc = 20\n",
        "    c1 = np.zeros(nvar)\n",
        "    c2 = np.zeros(nvar)\n",
        "    count = 0\n",
        "    for i in range(nvar):\n",
        "        nr = np.random.rand()\n",
        "        if nr < 0.5:\n",
        "            u = np.random.rand()\n",
        "            sn = 1e-16\n",
        "            if abs(indi1[i]-indi2[i]) > sn:\n",
        "                if indi1[i] < indi2[i]:\n",
        "                    p1 = indi1[i]\n",
        "                    p2 = indi2[i]\n",
        "                else:\n",
        "                    p1 = indi2[i]\n",
        "                    p2 = indi1[i]\n",
        "                \n",
        "                const1 = p1\n",
        "                const2 = 1-p2\n",
        "                \n",
        "                beta1 = 1 + 2*const1/(p2-p1)\n",
        "                beta2 = 1 + 2*const2/(p2-p1)\n",
        "                alpha1 = 2 - beta1**(-(nc+1))\n",
        "                alpha2 = 2 - beta2**(-(nc+1))\n",
        "                \n",
        "                if u <= (1/alpha1):\n",
        "                    beta_l1 = (alpha1*u)**(1/(nc+1))\n",
        "                else:\n",
        "                    beta_l1 = (1/(2-alpha1*u))**(1/(nc+1))\n",
        "                \n",
        "                c1[i] = 0.5*((p1+p2)-beta_l1*abs(p2-p1))\n",
        "                \n",
        "                if u <= (1/alpha2):\n",
        "                    beta_l2 = (alpha2*u)**(1/(nc+1))\n",
        "                else:\n",
        "                    beta_l2 = (1/(2-alpha2*u))**(1/(nc+1))\n",
        "                \n",
        "                c2[i] = 0.5*((p1+p2)+beta_l2*abs(p2-p1))\n",
        "\n",
        "            else:\n",
        "                c1[i] = indi1[i]\n",
        "                c2[i] = indi2[i]\n",
        "        else:\n",
        "            c1[i] = indi1[i]\n",
        "            c2[i] = indi2[i]\n",
        "        if(c1[i] < 0 or c1[i] > 1) or (c2[i] < 0 or c2[i] > 1):\n",
        "            count = count+1\n",
        "        if(count > 0):\n",
        "            print(count)\n",
        "\n",
        "    c = [c1,c2]\t\t\n",
        "    return c\n",
        "\n",
        "def crossover_process(matingpool,pcross):\n",
        "    npop,nvar = matingpool.shape\n",
        "    population = np.zeros((npop,nvar))\n",
        "    for jj in range(0,npop,2):\n",
        "        idx1 = np.random.randint(npop)\n",
        "        idx2 = np.random.randint(npop)\n",
        "        if idx2 == idx1:\n",
        "            while idx2 == idx1:\n",
        "                idx2 = np.random.randint(npop)\n",
        "        n = np.random.rand()\n",
        "        p1 = matingpool[idx1-1,:]\n",
        "        p2 = matingpool[idx2-1,:]\n",
        "        if(n < pcross):\n",
        "            child = SBX(p1,p2,nvar)\n",
        "            population[jj,:] = child[0]\n",
        "            population[jj+1,:] = child[1]\n",
        "        else:\n",
        "            population[jj,:] = p1\n",
        "            population[jj+1,:] = p2\n",
        "\n",
        "    return population\n",
        "\n",
        "print('Parent population:',population)\n",
        "pcross = 0.9\n",
        "population_cross = crossover_process(matingpool,pcross)\n",
        "print('\\nOffspring population:',population_cross)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsE4R9vmsOxt"
      },
      "source": [
        "---\n",
        "## Mutation\n",
        "* Mutation maintains the diversity of the population.\n",
        "* In this tutorial, **polynomial mutation** is utilized, expressed as follows:\n",
        "$$c_{j}^{'} = c_{j} + \\overline{\\delta} (x_{j,U}-x_{j,L})$$\n",
        "where\n",
        "$$\n",
        "\t\\overline{\\delta} = \\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t(2u+(1-2u)(1-\\delta)^{\\eta_m+1})^{\\frac{1}{\\eta_m+1}}-1, & \\text{if } u \\leq 0.5, \\\\\n",
        "\t1-(2(1-u)+2(u-0.5)(1-\\delta)^{\\eta_m+1})^{\\frac{1}{\\eta_m+1}}, & \\text{otherwise}.\n",
        "\t\\end{array}\n",
        "\t\\right.\n",
        "$$\n",
        "where $\\eta_m$ denotes the mutation distribution index. Here, we set the distribution index $\\eta_m=20$ for all experiments.\n",
        "\n",
        "<br><br>\n",
        "**Reference**:\n",
        "Deb, Kalyanmoy, and Debayan Deb. \"Analysing mutation schemes for real-parameter genetic algorithms.\" International Journal of Artificial Intelligence and Soft Computing 4, no. 1 (2014): 1-28."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBtZ5Bl6NF2u"
      },
      "source": [
        "#@title Mutation\n",
        "def polymut(var,nvar,pmut):\n",
        "    nm = 20\n",
        "    mutchrom = var.copy()\n",
        "    const = 1/(1+nm)\n",
        "    for i in range(nvar):\n",
        "        u = np.random.rand()\n",
        "        rn = np.random.rand()\n",
        "        if(rn < pmut):\n",
        "            delta = min(var[i],1-var[i])\n",
        "            if(u <= 0.5):\n",
        "              delta2 = (2*u+(1-2*u)*(1-delta)**(nm+1))**const - 1\n",
        "            else:\n",
        "              delta2 = 1 - (2*(1-u)+2*(u-0.5)*(1-delta)**(nm+1))**const\n",
        "            mutchrom[i] = mutchrom[i] + delta2\n",
        "        if (mutchrom[i] < 0):\n",
        "            mutchrom[i] = 0\n",
        "        if (mutchrom[i] > 1):\n",
        "            mutchrom[i] = 1\n",
        "    return mutchrom\n",
        "\n",
        "def mutation_process(population,pmut):\n",
        "\tnpop,nvar = population.shape\n",
        "\tpopulation_mut = population.copy()\n",
        "\tfor kk in range(npop):\n",
        "\t\tvar = population[kk,:]\n",
        "\t\tpopulation_mut[kk,:] = polymut(var,nvar,pmut)\n",
        "\t\n",
        "\treturn population_mut\n",
        "\n",
        "print('Before mutation:',population_cross)\n",
        "pmut = 0.8\n",
        "population_mut = mutation_process(population_cross,pmut)\n",
        "print('\\nAfter mutation:',population_mut)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TCAcA4KsVds"
      },
      "source": [
        "---\n",
        "## Integrating Real-Coded GA Code\n",
        "Finally, let's combine all of those EA mechanisms into a single integrated EA code! (let's call it ```soga```) For this demonstration , you can use either ackley or welded beam function. However, you can also define your own function too and try to optimize it by using this code.\n",
        "\n",
        "You need to define the number of population (```npop```), maximum generation (```maxgen```), crossover probability (```pcross```), and mutation probability (```pmut```). Feel free to change these parameters and see how they will affect the optimization process.\n",
        "\n",
        "Let's define the ```soga``` routine first by running the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNYnaP0aNFtP",
        "cellView": "form"
      },
      "source": [
        "#@title Integrated RGA\n",
        "def soga(fun_name,nvar,ncon,lb,ub,npop,maxgen,pcross,pmut,cht_type=None):\t\n",
        "\n",
        "    # Obtaining initial parent population (normalized form with value between 0 and 1)\n",
        "    pop_parent = np.random.rand(npop,nvar)\n",
        "      \n",
        "    # Calculating responses (objective and constraint values) of parent population\n",
        "    objval_parent,conval_parent = calc_obj_con(pop_parent,fun_name,ncon,lb,ub,cht_type)\n",
        "      \n",
        "    # Defining fitness value (can be real values, rank from MCR, etc) of parent poopulation\n",
        "    fitval_parent,cv_parent,Nv_parent = evaluate_fitval(objval_parent,conval_parent,cht_type)\n",
        "\n",
        "    # Finding feasible individuals of parent population\n",
        "    ind_parent_feas = np.where(Nv_parent==0)[0]\n",
        "\n",
        "    # Allocating lists for current best individual\n",
        "    best_individual = []\n",
        "    best_objval = []\n",
        "    best_conval = []\n",
        "    best_fitval = []\n",
        "\n",
        "    if len(ind_parent_feas) > 0:\n",
        "        pop_parent_feas = pop_parent[ind_parent_feas,:]\n",
        "        objval_parent_feas = objval_parent[ind_parent_feas]\n",
        "        conval_parent_feas = conval_parent[ind_parent_feas,:]\n",
        "        fitval_parent_feas = fitval_parent[ind_parent_feas]\n",
        "\n",
        "        # Sorting feasible individuals of parent population from lowest to highest fitness values\n",
        "        allIndex = np.argsort(fitval_parent_feas)\n",
        "\n",
        "        # Recording the current best individual\n",
        "        best_individual.append(pop_parent_feas[allIndex[0],:])\n",
        "        best_objval.append(objval_parent_feas[allIndex[0]])\n",
        "        best_conval.append(conval_parent_feas[allIndex[0],:])\n",
        "        best_fitval.append(fitval_parent_feas[allIndex[0]])\n",
        "    else:\n",
        "        # Sorting feasible individuals of parent population from lowest to highest fitness values\n",
        "        allIndex = np.argsort(fitval_parent)\n",
        "\n",
        "        # Recording the current best individual\n",
        "        best_individual.append(np.nan*np.ones(nvar))\n",
        "        best_objval.append(np.nan)\n",
        "        best_conval.append(np.nan*np.ones(ncon))\n",
        "        best_fitval.append(np.nan)\n",
        "\n",
        "    # Generational looping\n",
        "    for gen in range(maxgen):\n",
        "        # if gen == 0:\n",
        "        #     print('currently evaluating gen:',gen,', current optimum: ',objval_parent_feas[allIndex[0]])\n",
        "        # else:\n",
        "        #     print('currently evaluating gen:',gen,', current optimum: ',objval_child_feas[allIndex[0]])\n",
        "\n",
        "        # Stopping criterion\n",
        "        if gen == maxgen-1:\n",
        "          break\n",
        "        \n",
        "        # Creating mating pool\n",
        "        matingpool = create_matingpool(pop_parent,fitval_parent)\n",
        "        \n",
        "        # Conducting crossover\n",
        "        pop_child = crossover_process(matingpool,pcross)\n",
        "        \n",
        "        # Conducting mutation\n",
        "        pop_child_mut = mutation_process(pop_child,pmut)\n",
        "        \n",
        "        # Calculating responses (objective and constraint values) of child population\n",
        "        objval_child,conval_child = calc_obj_con(pop_child_mut,fun_name,ncon,lb,ub,cht_type)\n",
        "\n",
        "        # Elitism\n",
        "        if len(ind_parent_feas) > 0:\n",
        "            pop_child_mut[0,:] = best_individual[len(best_individual)-1]\n",
        "            objval_child[0] = best_objval[len(best_objval)-1]\n",
        "            conval_child[0,:] = best_conval[len(best_conval)-1]\n",
        "        else:\n",
        "            pop_child_mut[0,:] = pop_parent[allIndex[0],:]\n",
        "            objval_child[0] = objval_parent[allIndex[0]]\n",
        "            conval_child[0,:] = conval_parent[allIndex[0],:]\n",
        "\n",
        "        # Defining fitness value (can be real values, rank from MCR, etc) of combined poopulation\n",
        "        fitval_child,cv_child,Nv_child = evaluate_fitval(objval_child,conval_child,cht_type)\n",
        "\n",
        "        # Finding feasible individuals of child population\n",
        "        ind_child_feas = np.where(Nv_child==0)[0]\n",
        "\n",
        "        if len(ind_child_feas) > 0:\n",
        "            pop_child_feas = pop_child[ind_child_feas,:]\n",
        "            objval_child_feas = objval_child[ind_child_feas]\n",
        "            conval_child_feas = conval_child[ind_child_feas,:]\n",
        "            fitval_child_feas = fitval_child[ind_child_feas]\n",
        "\n",
        "            # Sorting individuals of combined population from lowest to highest fitness values\n",
        "            allIndex = np.argsort(fitval_child_feas)\n",
        "\n",
        "            # Recording the current best individual\n",
        "            best_individual.append(pop_child_feas[allIndex[0],:])\n",
        "            best_objval.append(objval_child_feas[allIndex[0]])\n",
        "            best_conval.append(conval_child_feas[allIndex[0],:])\n",
        "            best_fitval.append(fitval_child_feas[allIndex[0]])\n",
        "        else:\n",
        "            # Sorting individuals of combined population from lowest to highest fitness values\n",
        "            allIndex = np.argsort(fitval_child)\n",
        "\n",
        "            # Recording the current best individual\n",
        "            best_individual.append(np.nan*np.ones(nvar))\n",
        "            best_objval.append(np.nan)\n",
        "            best_conval.append(np.nan*np.ones(ncon))\n",
        "            best_fitval.append(np.nan)\n",
        "\n",
        "        # New population\n",
        "        pop_parent = pop_child_mut.copy()\n",
        "        fitval_parent = fitval_child.copy()\n",
        "        objval_parent = objval_child.copy()\n",
        "        conval_parent = conval_child.copy()\n",
        "        Nv_parent = Nv_child.copy()\n",
        "\n",
        "        # Finding feasible individuals of parent population\n",
        "        ind_parent_feas = np.where(Nv_parent==0)[0]\n",
        "\n",
        "    best_individual = np.array(best_individual)\n",
        "    best_objval = np.array(best_objval)\n",
        "    best_conval = np.array(best_conval)\n",
        "\n",
        "    result = [best_objval,best_individual,best_conval]\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeRBuTwOsxx5"
      },
      "source": [
        "Let's try it for solving the Ackley and the Welded beam problem (please change ```fun_name```):\n",
        "\n",
        "Note: for the Ackley problem, you can decrease or increase the number of variables (```nvar```) to make it less or more difficult, respectively. Further, you can change the population size (```npop```), maximum generations(```maxgen```), crossover probability (```pcross```) and mutation probability(```pmut```). We suggest you to play around with these parameters and see how they affect the optimization process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liMAUny8NFiV"
      },
      "source": [
        "#@title Running RGA\n",
        "#=========================================================================\n",
        "# GA parameters and search space\n",
        "\n",
        "fun_name = 'ackley'\n",
        "# fun_name = 'weldedbeam'\n",
        "\n",
        "npop = 100 # Population size\n",
        "maxgen = 100 # maximum generations\n",
        "pcross = 0.9 # Crossover probability\n",
        "pmut = 0.1 # Mutation probability\n",
        "\t\n",
        "if fun_name == 'ackley':\n",
        "    nvar = 2\n",
        "    ncon = 0\n",
        "    cht_types = None\n",
        "    lb = -32.768*np.ones(nvar)\n",
        "    ub = 32.768*np.ones(nvar)\n",
        "elif fun_name == 'weldedbeam':\n",
        "    nvar = 4\n",
        "    ncon = 6\n",
        "    #cht_types = 'SoF'\n",
        "    cht_types = 'G-MCR'\n",
        "    lb = np.array([0.125,0.1,0.1,0.1])\n",
        "    ub = np.array([2,10,10,2])\n",
        "\n",
        "# ========================================================================\n",
        "# Optimization - Trial\n",
        "result = soga(fun_name,nvar,ncon,lb,ub,npop,maxgen,pcross,pmut,cht_type=cht_types)\n",
        "best_objval = result[0]\n",
        "best_individual = result[1]\n",
        "best_conval = result[2]\n",
        "\n",
        "print('Objective value of the best solution: ',best_objval[len(best_objval)-1])\n",
        "print('Decision variables of the best solution: ',best_individual[len(best_objval)-1])\n",
        "print('Constraint values of the best solution: ',best_conval[len(best_objval)-1])\n",
        "\n",
        "# ========================================================================\n",
        "# Plotting the convergence performance\n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(0,maxgen),best_objval)\n",
        "plt.xlabel('Generation')\n",
        "plt.ylabel('Obtained Minimum Value')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH02tqwjs8YW"
      },
      "source": [
        "You should see the obtained minimum value decreases as the number of generation increases, which indicates the success of the EA search! Maybe you also want to try experimenting with the parameters or change the CHT. Note that you can also increase the dimensionality of the Ackley function to make the problem harder (and how would you solve that? What parameters that you need to change?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9uE-UbjbxTb"
      },
      "source": [
        "---\n",
        "# Statictical Analysis with Box Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjtdkqfYuNhl"
      },
      "source": [
        "Especially when you do research in the field of EA, you need to perform statistical analysis to compare the performance of various EA methods (or different crossover techniques, let's say). By that, we mean that you should run EA multiple times because of the random nature of EA. What you want to analyze is then the average of the performance of EA, not its performance in a single run. Notice that you need to increase the number of trials to obtain a more statistically valid result. However, remember that increasing the number of trials also increases the computation time.\n",
        "\n",
        "* As explained in the theory overview, stochastic optimization methods such as EA require statistical analysis to ensure their consistency in terms of performance. \n",
        "* In this tutorial, we will conduct the statistical analysis visually with a box plot. Remember that usually we need to do a statistical hypothesis test to further verify the performance.\n",
        "* As EA requires some user-defined settings, let's check its performance sensitivity by changing the basic parameters!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg6vGncJhWjw"
      },
      "source": [
        "First, let's try varying the population size. How would different population sizes affect the EA search? With a fixed number of maximum evals, should we (1) deploy a lower population size with more generations, or (2) higher population size with lower generations?. Here, you can perform the experiment by changing the following list, e.g. ```npops = [300,30]```, which means that you will experiment with $N_{pop}=300$ and $N_{pop}=30$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQMYrBDtcBxj"
      },
      "source": [
        "#@title Statistical Analysis - Npop\n",
        "# ========================================================================\n",
        "# Setting number of trials\n",
        "n_try = 5 # Number of trials (it takes time for more trials)\n",
        "\n",
        "#=========================================================================\n",
        "# Optimization - Npop Sensitivity\n",
        "\n",
        "#fun_name = 'ackley'\n",
        "fun_name = 'weldedbeam'\n",
        "\n",
        "if fun_name == 'ackley':\n",
        "    nvar = 2\n",
        "    ncon = 0\n",
        "    cht_types = None\n",
        "    lb = -32.768*np.ones(nvar)\n",
        "    ub = 32.768*np.ones(nvar)\n",
        "elif fun_name == 'weldedbeam':\n",
        "    nvar = 4\n",
        "    ncon = 6\n",
        "    #cht_types = 'SoF'\n",
        "    cht_types = 'G-MCR'\n",
        "    lb = np.array([0.125,0.1,0.1,0.1])\n",
        "    ub = np.array([2,10,10,2])\n",
        "\n",
        "npops = [300,30] # Please make the list of various number of populations here\n",
        "maxeval = 30000 # The number of maximum function evaluations, your number of generation is then maxeval/npop\n",
        "pcross = 0.9 # Crossover probability\n",
        "pmut = 0.1 # Mutation probability\n",
        "\n",
        "collect_all_objval = []\n",
        "for i_pop in range(len(npops)):\n",
        "    npop = npops[i_pop]\n",
        "    maxgen = int(maxeval/npop)\n",
        "\n",
        "    all_objval = np.zeros(n_try)\n",
        "    for i_try in range(n_try):\n",
        "        \n",
        "        print('currently running npop: ',npop,' run: ',i_try)\n",
        "        \n",
        "        result = soga(fun_name,nvar,ncon,lb,ub,npop,maxgen,pcross,pmut,cht_type=cht_types)\n",
        "        best_objval = result[0]\n",
        "        best_individual = result[1]\n",
        "        best_conval = result[2]\n",
        "        \n",
        "        all_objval[i_try] = best_objval[len(best_objval)-1]\n",
        "      \n",
        "    collect_all_objval.append(all_objval)\n",
        "\n",
        "# Box plot of Population Size Sensitivity\t\n",
        "fig,ax = plt.subplots()\n",
        "ax.boxplot(collect_all_objval)\n",
        "plt.xlabel('Population Size')\n",
        "plt.ylabel('Optimum Objective Value')\n",
        "plt.xticks(range(1,len(npops)+1),npops)\n",
        "ax.set_xticklabels(npops, rotation=45, ha='right')\n",
        "plt.grid(which='major')\n",
        "plt.grid(which='minor',linestyle='-.',alpha=0.2)\n",
        "plt.minorticks_on()\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IijRjD_Z521I"
      },
      "source": [
        "Let's try varying the crossover probability. How would different crossover probability affect the EA search? Should we set a high or maybe a lower value of crossover probability? Do the experiment by changing the values of the crossover probability in ```pcrosses```, which is a list, e.g. ```npops = [0.9,0.1]```, which means that you will perform experiment with $P_{cross}=0.9$ and $P_{cross}=0.1$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwNs2xLgh6Zz"
      },
      "source": [
        "#@title Statistical Analysis - Pcross\n",
        "#=========================================================================\n",
        "# Setting number of trials\n",
        "n_try = 5\n",
        "\n",
        "#=========================================================================\n",
        "# Optimization - Pcross Sensitivity\n",
        "fun_name = 'ackley'\n",
        "#fun_name = 'weldedbeam'\n",
        "\n",
        "if fun_name == 'ackley':\n",
        "    nvar = 2\n",
        "    ncon = 0\n",
        "    cht_types = None\n",
        "    lb = -32.768*np.ones(nvar)\n",
        "    ub = 32.768*np.ones(nvar)\n",
        "elif fun_name == 'weldedbeam':\n",
        "    nvar = 4\n",
        "    ncon = 6\n",
        "    cht_types = 'SoF'\n",
        "    lb = np.array([0.125,0.1,0.1,0.1])\n",
        "    ub = np.array([2,10,10,2])\n",
        "\n",
        "npop = 100 # Population size\n",
        "maxgen = 300 # Number of maximum generations\n",
        "pcrosses = [0.9,0.1] # Please make the list of various crossover probability here\n",
        "pmut = 0.1 # Mutation probability\n",
        "\n",
        "collect_all_objval2 = []\n",
        "for i_cross in range(len(pcrosses)):\n",
        "\tpcross = pcrosses[i_cross]\n",
        "\tall_objval2 = np.zeros(n_try)\n",
        "\tfor i_try in range(n_try):\n",
        "\t\t\n",
        "\t\tprint('currently running pcross: ',pcross,' run: ',i_try)\n",
        "\t\t\n",
        "\t\tresult = soga(fun_name,nvar,ncon,lb,ub,npop,maxgen,pcross,pmut,cht_type=cht_types)\n",
        "\t\tbest_objval = result[0]\n",
        "\t\tbest_individual = result[1]\n",
        "\t\tbest_conval = result[2]\n",
        "\t\t\n",
        "\t\tall_objval2[i_try] = best_objval[len(best_objval)-1]\n",
        "\t\t\n",
        "\tcollect_all_objval2.append(all_objval2)\n",
        "\n",
        "# Box plot of Crossover Probability Sensitivity\t\n",
        "fig,ax = plt.subplots()\n",
        "ax.boxplot(collect_all_objval2)\n",
        "plt.xlabel('Crossover Probability')\n",
        "plt.ylabel('Optimum Objective Value')\n",
        "plt.xticks(range(1,len(pcrosses)+1),pcrosses)\n",
        "ax.set_xticklabels(pcrosses, rotation=45, ha='right')\n",
        "plt.grid(which='major')\n",
        "plt.grid(which='minor',linestyle='-.',alpha=0.2)\n",
        "plt.minorticks_on()\n",
        "plt.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxUKRtRzhw8w"
      },
      "source": [
        "Next, let's try varying the mutation probability. How would different mutation probabilities affect the EA search? What would happen if we set a mutation probability that is too high or maybe too low? Experiment by changing the values of the mutation probability in ```pmuts```, which is a list, e.g. ```npops = [0.9,0.1]```, which means that you will perform experiment with $P_{mut}=0.9$ and $P_{mut}=0.1$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXhN__NxiLmh"
      },
      "source": [
        "#@title Statistical Analysis - Pmut\n",
        "#=========================================================================\n",
        "# Setting number of trials\n",
        "n_try = 5\n",
        "\n",
        "#=========================================================================\n",
        "# Optimization - Pmut Sensitivity\n",
        "fun_name = 'ackley'\n",
        "#fun_name = 'weldedbeam'\n",
        "\n",
        "if fun_name == 'ackley':\n",
        "    nvar = 2\n",
        "    ncon = 0\n",
        "    cht_types = None\n",
        "    lb = -32.768*np.ones(nvar)\n",
        "    ub = 32.768*np.ones(nvar)\n",
        "elif fun_name == 'weldedbeam':\n",
        "    nvar = 4\n",
        "    ncon = 6\n",
        "    cht_types = 'SoF'\n",
        "    lb = np.array([0.125,0.1,0.1,0.1])\n",
        "    ub = np.array([2,10,10,2])\n",
        "\n",
        "npop = 100 # Population size\n",
        "maxgen = 300 # Maximum generation\n",
        "pcross = 0.9 # Crossover probability\n",
        "pmuts = [0.9,0.1] # Please make the list of various mutation probability here\n",
        "\n",
        "collect_all_objval3 = []\n",
        "for i_mut in range(len(pmuts)):\n",
        "\tpmut = pmuts[i_mut]\n",
        "\tall_objval3 = np.zeros(n_try)\n",
        "\tfor i_try in range(n_try):\n",
        "\t\t\n",
        "\t\tprint('currently running pmut: ',pmut,' run: ',i_try)\n",
        "\t\t\n",
        "\t\tresult = soga(fun_name,nvar,ncon,lb,ub,npop,maxgen,pcross,pmut,cht_type=cht_types)\n",
        "\t\tbest_objval = result[0]\n",
        "\t\tbest_individual = result[1]\n",
        "\t\tbest_conval = result[2]\n",
        "\t\t\n",
        "\t\tall_objval3[i_try] = best_objval[len(best_objval)-1]\n",
        "\t\t\n",
        "\tcollect_all_objval3.append(all_objval3)\n",
        "\n",
        "# Box plot of Mutation Probability Sensitivity\n",
        "fig2,ax2 = plt.subplots()\n",
        "ax2.boxplot(collect_all_objval3)\n",
        "plt.xlabel('Mutation Probability')\n",
        "plt.ylabel('Optimum Objective Value')\n",
        "plt.xticks(range(1,len(pmuts)+1),pmuts)\n",
        "ax2.set_xticklabels(pmuts, rotation=45, ha='right')\n",
        "plt.grid(which='major')\n",
        "plt.grid(which='minor',linestyle='-.',alpha=0.2)\n",
        "plt.minorticks_on()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH56vCxTZzI3"
      },
      "source": [
        "Finally, let's study the effect of constraint handling. We will compare the conventional SoF method with G-MCR, the latter is one of state-of-the-art CHT techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMEpqo0iokxx"
      },
      "source": [
        "#@title Statistical Analysis - CHT\n",
        "# ========================================================================\n",
        "# Setting number of trials\n",
        "n_try = 5\n",
        "\n",
        "#=========================================================================\n",
        "# Optimization - CHT Comparison\n",
        "\n",
        "\n",
        "fun_name = 'weldedbeam'\n",
        "nvar = 4\n",
        "ncon = 6\n",
        "lb = np.array([0.125,0.1,0.1,0.1])\n",
        "ub = np.array([2,10,10,2])\n",
        "\n",
        "cht_types = ['SoF','G-MCR']\n",
        "npop = 30\n",
        "maxeval = 30000\n",
        "pcross = 0.9\n",
        "pmut = 0.1\n",
        "\n",
        "collect_all_objval = []\n",
        "for i_cht in range(len(cht_types)):\n",
        "    cht_type = cht_types[i_cht]\n",
        "    maxgen = int(maxeval/npop)\n",
        "\n",
        "    all_objval = np.zeros(n_try)\n",
        "    for i_try in range(n_try):\n",
        "        \n",
        "        print('currently running CHT: ',cht_type,' run: ',i_try)\n",
        "        \n",
        "        result = soga(fun_name,nvar,ncon,lb,ub,npop,maxgen,pcross,pmut,cht_type=cht_type)\n",
        "        best_objval = result[0]\n",
        "        best_individual = result[1]\n",
        "        best_conval = result[2]\n",
        "        \n",
        "        all_objval[i_try] = best_objval[len(best_objval)-1]\n",
        "      \n",
        "    collect_all_objval.append(all_objval)\n",
        "\n",
        "# Box plot of Population Size Sensitivity\t\n",
        "fig,ax = plt.subplots()\n",
        "ax.boxplot(collect_all_objval)\n",
        "plt.xlabel('Population Size')\n",
        "plt.ylabel('Optimum Objective Value')\n",
        "plt.xticks(range(1,len(cht_types)+1),cht_types)\n",
        "ax.set_xticklabels(cht_types, rotation=45, ha='right')\n",
        "plt.grid(which='major')\n",
        "plt.grid(which='minor',linestyle='-.',alpha=0.2)\n",
        "plt.minorticks_on()\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}